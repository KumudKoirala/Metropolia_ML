{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f396427f",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12811cbf",
   "metadata": {},
   "source": [
    "Artificial Intelligence(AI) can be defined as the process of automating the task which are normally performed by humans.As shown in figure 1.1 , Machine Learning(ML) and Deep Learning(DL) are the subset of AI. DL is the subset of the Machine learning which is used to solve the practical tasks in variety of fields such as in computer vision ,natural language processing and automatic speech recognition.Deep Learning uses the methods that are available in Machine Learning whose algorithm are based on how human brain works. \n",
    "The brain is the organ which dictates the way we perceive the signal from our sensory organs such as sight,sound,smell,taste and touch. Based on the signal we are able to make some actions. We have been dreaming of building an intelligent machines which works like our brain.Our brain can take sensory signal as an input and make a decision in matter of microseconds. However an Artificially intelligent machines requires us to solve some of the most complex computational problem. Deep Learning helps us to solve this problem by programming a computer in a different way using the traditional techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39918a2",
   "metadata": {},
   "source": [
    "![figure1_1.png](images/figure1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aad48b",
   "metadata": {},
   "source": [
    "# Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafaee73",
   "metadata": {},
   "source": [
    "A neuron or the nerve cell  is the basic building blocks of the nervous system(Figure 1.2). A neuron is optimized to receive and process information from other neurons and send the result to other cells. The neuron receives its inputs along the root like structure called dendrites. Each dendrites of the neurons are indirectly connected to other neuron at the axon terminal by leaving a small spaces called synapses. Each of these signal received by dendrites are dynamically strengthed or weakened based upon how often it is use.Each of these inputs are weighted based upon strength and summed together and transformed into a new signal which is propagates through cell axon and transmitted to other neurons through synapses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a978db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T09:18:29.243173Z",
     "start_time": "2022-05-05T09:18:29.157165Z"
    }
   },
   "source": [
    "![figure1_2.png](images/figure1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e7aef",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9e7ab",
   "metadata": {},
   "source": [
    "Multilayer perceptron is a  fully connected class of feedforward artificial network. A feed forward network is an Artficial Neural network where the connections between the nodes do not form any cycle. It is one of the simplest network having one input layer , one output layer and few hidden layers. Each layer has multiple neurons and the adjacent layers are fully connected. Weighted signals from the previous layer are push towards neuron located at next layer.The weighted signals are summed up at each neurons and bias is added to sum of products at each neurons. An activation function maps this weighted sum of product(SOP) so as to decide whether or not this SOP needs to be considered. This results an ouput signals for the next level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb7ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T10:22:56.192956Z",
     "start_time": "2022-05-05T10:22:56.145954Z"
    }
   },
   "source": [
    "![figure1_3.png](images/figure1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1709b",
   "metadata": {},
   "source": [
    "An input layer consists of raw data or preprocessed data. For example , images as an input would be the raw pixel values or for a text data it could be the word or characters.\n",
    "\n",
    "An output layer is the output value of the network and depends upon the type of problem we are dealing with. For example for a classification problems, the output layer can have $n$ neurons for n number of classes and uses softmax function to output the probability of being in each class.For regression problem , network can have 1 ouput so as to predict a single quantity. Figure1.3 depicts a fully connected neural network with two hidden layers , 3 inputs and 1 output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b076f",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb611df2",
   "metadata": {},
   "source": [
    "An activation function in each artificial neuron present at each layers decides whether the incoming signals have reached the threshold and should output signals for the next level.A neural network has different activation functions as shown in the matplot's plot below.An activation function maps received weighted values sum (SOP) in a non linear way . For example, the activation function(ReLU) can release a 0 value unless the input achieves a certain threshold or (Leaky-Relu) it can dampen a value by some scaling factor until a threshold is achieved.\n",
    "Neural Networks commonly uses sigmoid ,hyperbolic tangent(TanH) or the Relu().A good activation functions are non linear, i.e they turn at some point. Sigmoid function(see plot below) squishes the infinite amount of input to an output between 0 and 1.Tanh is same as sigmoid except it maps everthing in between -1 and 1.  \n",
    "Take a look at some of the activation function below and try to think what kind of function reLU and Leaky-reLu are ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156f08c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T07:46:04.415024Z",
     "start_time": "2022-05-24T07:46:04.099029Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f089957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T07:46:04.445000Z",
     "start_time": "2022-05-24T07:46:04.417001Z"
    }
   },
   "outputs": [],
   "source": [
    "class activation_fun():\n",
    "    def __init__(self,x):\n",
    "        self.x=x\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        return 1/(1+np.exp(-1*self.x))\n",
    "    \n",
    "    def tanh(self):\n",
    "        return np.divide ((np.exp(self.x)-np.exp(-1*self.x)), (np.exp(self.x)+np.exp(-1*self.x)))\n",
    "    \n",
    "    def relu(self):\n",
    "        return [ 0 if ele<0 else ele for ele in self.x ] \n",
    "    \n",
    "    def leaky_relu(self):\n",
    "        return [ 0.15*ele if ele<0 else ele for ele in self.x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54331fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T07:46:04.648344Z",
     "start_time": "2022-05-24T07:46:04.642342Z"
    }
   },
   "outputs": [],
   "source": [
    "active_func=activation_fun(np.arange(-6,6,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c597211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T07:46:05.873481Z",
     "start_time": "2022-05-24T07:46:05.864478Z"
    }
   },
   "outputs": [],
   "source": [
    "func_methods=[(lambda m: m.sigmoid()),(lambda m:m.tanh()),(lambda m:m.relu()),(lambda m:m.leaky_relu())]\n",
    "func_title=['sigmoid','tanh','relu','leaky_relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a668a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T07:46:06.769165Z",
     "start_time": "2022-05-24T07:46:06.416488Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,2,figsize=(10,8),facecolor='w',edgecolor='k')\n",
    "for i, (ttl,f,ax) in enumerate(zip(func_title,func_methods,axs.ravel())):\n",
    "    ax.plot(np.arange(-6,6,0.5),f(active_func))\n",
    "    ax.set_title(ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3efda31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T10:10:39.786610Z",
     "start_time": "2022-05-05T10:10:39.769644Z"
    }
   },
   "source": [
    "## How A Network Learns ? Forward Propagation and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2638a7",
   "metadata": {},
   "source": [
    "Figure 1.4 shown below is a computational graph representation for a neural network represented in figure 1.3. We are going to present the both operation forward and backward propagation based on that model. Let us assume the sigmoid function ($\\sigma$ ) as our activation function. \n",
    "When the network is configured, training of the network starts with some randomized weights. The weights can be propely initialized as well so that the coefficients can best capture the data patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a5953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T14:20:20.747207Z",
     "start_time": "2022-05-05T14:20:20.692760Z"
    }
   },
   "source": [
    "![figure1_4.png](images/figure1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f82fb1",
   "metadata": {},
   "source": [
    "In the figure above $X$ is the input vector of order of length $M = 3$(or could be a Matrix of order $N \\times  M$ where $N$ is the total number of samples).$W_1$ and $W_2$ is the weight matrix and $b_1$ and $b_2$ is the bias vector at layer 1  and layer 2 respectively . An activation function maps input $h_1$ and outs $h_2$. $\\hat{y}$ is the predicted output and $t$ is the true target values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031edb44",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67270a74",
   "metadata": {},
   "source": [
    "Forward propagation is the process of calculating the input data multiplied by the networks weight plus added bias. This sum of products with added bias are then mapped with some activation function. This process of generating output is also a part of prediction mapping which is carrried out by a prediction map $h(x)$ such that predicted output is $\\hat y=h(x) $. A linear predictor map has the form of $h(x)=w^Tx=\\sum_{i=1}^d w_ix_i $ . In the example below, we may consider the output of the matrix multiplication between input and weight is the output of the prediction Map. \n",
    "\n",
    "Let us recall linear algebra for two matrix multiplication. In order to multiply two matrices $A$ and $B$, the number of columns in first matrix $A$ must be equal to the number of rows in 2nd matrix $B$.  The resultant matix order will be the number of rows in 1st matrix $A$ and number of columns in 2nd matrix.Let us calculate the size of the Input matrix, output matrix at all layers and all the intermediate weights.\n",
    "\n",
    "\n",
    "- $X$ is of order $1 \\times 3 $ .For simplicity,assume $X$ consists of a 1 sample. If there are N samples then order will be of $N\\times3 $\n",
    "- $W_1$ is of order $3\\times4$ -- We have 4 neurons in layer 1 and the size of input is 3 . \n",
    "\n",
    "Therefore \n",
    "$$ order-of-weight= $length-of-input \\times  number-of-neurons$$\n",
    "\n",
    "- $h_1$ is the matrix multiplication result between $X$.$W_1$ and after adding bias $b_1$ () . The resultant matrix size ($X$.$W_1$) would be($1\\times4$)and therfore the bias vector size should be ($1\\times4$).\n",
    "\n",
    "- $h_2$ is the output of activation function(sigmoid).$h_2=\\sigma(h_1)$.\n",
    "\n",
    "- $h_2$ is the order of ($1\\times4$) which is the input for layer 2. the number of input =4  and number of neurons in layer 2 =4 . Therfore order of weight $W_2=4\\times4$ and the resultant output is $1\\times4$. This would be added with the bias size of $1\\times4$.\n",
    "\n",
    "- At the output layer we have 1 output $\\hat{y}$ and the input size is  $1\\times4$ so the order of weight matrix should be \n",
    "  $4\\times1$. The order of output matrix which is $\\hat{y}$ would be scalar consisting of only one element. Similary the bias size would be 1.\n",
    "  \n",
    "- (parameter that a network learns during training)No. of trainable parameters = sum of all the elements present in the each weight matrix + length of each biases\n",
    "                              =( total elements in  $W_1$ + total elements in  $W_2$ + total elements in weight matrix at output layer + length of bias $b_1$  + length of bias $b_2$+ length of bias at output layer )\n",
    "                              =  12+16+4+4+4+1 = 41\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f7c15",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255b751",
   "metadata": {},
   "source": [
    "Neural network learns from the error and updates the network weight or the parameters based on a given cost function. For the computational graph in Figure 1.4 , we take Mean Square Error  as our cost function(loss function).Mean square loss is defined as \n",
    "$$c=\\frac{1}{N}\\sum_{i=1}^N (y_i - t_i)^2 $$\n",
    "If we assume there are $N$ samples in  $X$ consisting of 3 features(length of $X$), we calculate average MSE. Therefore we have extra term $\\frac{1}{N}$.$i$ represents the index for the each sample processed. For example $i=0$ corresponds to the 0th sample, $i=1$ corresponds to the 1st sample and so on .$y_i$ and $t_i$is the predicted output and target value of the corresponding $i^{\\text{th}}$ input sample in $X$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac92a2d",
   "metadata": {},
   "source": [
    "Let us consider a composite function $F(x) =f(g(x))$ also written as  $F(x)=f \\circ g$ . The derivative of function $F(x)$ can be calculated using popular chain-rule method and can be written as \n",
    "$$F'(x)=f'(g(x))g'(x)$$\n",
    "The symbol <b>'</b> indicates the first order derivative of the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21245d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T10:15:21.525654Z",
     "start_time": "2022-05-09T10:15:21.466655Z"
    }
   },
   "source": [
    "![figure1_5.png](images/figure1_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461372a",
   "metadata": {},
   "source": [
    "Consider a Multi layer perceptron consisting . The target is to find the gradient of loss with respect to each learnable parameters in the neural network. For the computational graph shown in figure 1.5 ,we can define \n",
    "$$ h= f_1(X,W) +b_1 $$\n",
    "$$ y= f_2(h,\\theta) +b_2 $$\n",
    "$$f_1(X,W)=X.W $$\n",
    "$$f_2(h,\\theta)=h.\\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09201423",
   "metadata": {},
   "source": [
    "For a cost function such as MSE in equation(2) , we can write the gradient of loss function with respect to(wrt) output $y$ \n",
    "$$\\frac{\\partial }{\\partial y} = \\frac{2*(y_i-t_i)}{N} $$\n",
    "The gradient of loss wrt to input parameter $\\theta$ is given by (Using chain rule)\n",
    "$$\\frac{\\partial ℒ}{\\partial \\theta} =\\frac{\\partial ℒ}{\\partial y} \\frac{\\partial y}{\\partial \\theta}  $$\n",
    "The gradient of loss wrt to input parameter $w$ is given by (using chain rule )\n",
    "$$\\frac{\\partial ℒ}{\\partial w} =\\frac{\\partial ℒ}{\\partial y} \\frac{\\partial y}{\\partial h} \\frac{\\partial h}{\\partial w}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c092e",
   "metadata": {},
   "source": [
    "The partial derivative $\\frac{\\partial y}{\\partial h}$ ,$\\frac{\\partial h}{\\partial w}$ and $\\frac{\\partial y}{\\partial \\theta}$ can be easily calculated using equation 4,5 ,6 and 7. The reason it is called back propagation is because we are able to represent our gradient of loss wrt to all the trainabale parameters in terms of partial derivative of output wrt to input at every intermediate layers. We calculate each partial derivative starting from the back(ouput) and move towards starting point (input) as shown in figure 1.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32f002",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE):LOSS FUNCTION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d3f13",
   "metadata": {},
   "source": [
    "Mean square Error(in equation(2)) is one of the most used loss function for machine learning.The squared loss is defined as \n",
    "$$ ℒ(y,\\hat{y}) =   (y-\\hat{y})^2 $$ \n",
    "where $y$ and $\\hat{y}$ is the target and predicted target values respectively.\n",
    "The average of this square loss is termed as MSE can be written as \n",
    "$$ f(w) = \\frac{1}{n} [(y_0-\\hat{y_0})^2 + (y_1-\\hat{y_1})^2 + ........................+ (y_n-\\hat{y_n})^2]  $$\n",
    "The loss function $f(w)$ is the function of weight $w$ which measures the loss ooccured by prediction of target label $\\hat y$ and true target label $y$.By choosing the linear predictor with MSE  results in a loss function which has convex shape. The convex function are differentiable and the local minima for a convex function is always a global minima.\n",
    "In case a predictor has non linear dependencies on weights, we would have non convex shape of MSE.So our algorithm might stuck in local minima which is not a global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c2ec4",
   "metadata": {},
   "source": [
    "# Gradient Descent (GD) and Stochastic Gradient Descent(SGD): An Optimization Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907295d",
   "metadata": {},
   "source": [
    "Optimization is the process how a network learns.Learning is an optimization process which minimizes the error ,cost or finds the locus of the least errors. It then adjusts or updates  the network coefficient.Gradient Descent is one of the optimization algorithms which  performs a prediction and weight update by running through all the samples in the training dataset for a single weight  update. Our gradient descent moves towards the negative direction of gradient of loss function $\\nabla f(w)$.\n",
    "Gradient Descent finds the sequence of weight vectors such that the loss values tends towards minimum. For this purpose an initial guess would be made for the weights which changes iteratively so that our loss values converges towards zero.  \n",
    "The weight update algorithm given the gradient of loss function moving with step $\\epsilon $ is given as:\n",
    "\n",
    "$$W_{t+1}=W_{t}-\\epsilon \\nabla f(w)_{t} $$\n",
    "\n",
    "The next weight(updated weight) $W_{t+1}$ depends upon the current weight $W_{t}$ ,$\\epsilon$, and current gradient of loss function $\\nabla f(w)_{t}$. Here epsilon is the step size which actually controls how quicker do you want to move towards the minima of loss function . Moving too fast may result in the skip of minima or moving too slow may result in larger time to reach to convergence.Now let's look at the equation (10) of backward propagation. The backward propagation calculates the gradient at each layer and the new weights are updated using equation (13). Figure(1.6), shows the overall process of Deep Learning work flow.  \n",
    "\n",
    "GD has a disadvantage specially when we have large sets of data because we have to wait for huge amount of time just to make a single update to the current weight. In order to avoid this computational complexity , Stochastic Gardient Descent(SGD) uses small subset known as batch as a training datset. \n",
    "\n",
    "Full Gradient Descent is another improvisation to SGD where the network calculates the average weight over the entire dataset, changing the weights only each time it computes a full average. \n",
    "\n",
    "Batch Gradient Descent in other hand selects the sample randomly of size typically ranging from 8 to 256  and the weights are updated for each batch. This technique makes optimization fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b69c8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T05:57:22.353814Z",
     "start_time": "2022-05-24T05:57:22.296815Z"
    }
   },
   "source": [
    "![figure1_6.png](images/figure1_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84cfcd",
   "metadata": {},
   "source": [
    "# MLP for Training MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321e24fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:31:21.480590Z",
     "start_time": "2022-10-31T17:31:17.949313Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import datasets,layers,models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.datasets  import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2f713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:57:56.289816Z",
     "start_time": "2022-09-28T15:57:56.085611Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e92ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:57:56.305612Z",
     "start_time": "2022-09-28T15:57:56.291612Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'The shape of x_train is :{x_train.shape}') ## train data\n",
    "print(f'The shape of x_test is :{x_test.shape}') ## test data \n",
    "print(f'The shape of y_train is :{y_train.shape}') ##  train labels \n",
    "print(f'The shape of y_test is :{y_test.shape}')## test labels \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15daf014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T16:08:18.313890Z",
     "start_time": "2022-09-28T16:08:18.301854Z"
    }
   },
   "outputs": [],
   "source": [
    "len_of_class = len(np.unique(y_train))\n",
    "print(len_of_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae59f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T16:03:09.378637Z",
     "start_time": "2022-09-28T16:03:09.096642Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,5,figsize=(10,8),facecolor='w',edgecolor='k')\n",
    "for image,label, ax in zip(x_train,y_train,axs.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f'The number is {label}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c7f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T16:15:26.350556Z",
     "start_time": "2022-09-28T16:15:26.303624Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                   layers.Rescaling(1./255, input_shape=(28,28,1)),  # rescaling normalizes the pixel value between 0 and 1. Th\n",
    "                                                                    # Maximum pixel size being 255 \n",
    "                   layers.Flatten(), # Flattening changes tensor to 1D array\n",
    "                   layers.Dense(256,activation='relu'), # first layer of MLP consists of 256 neurons\n",
    "                   layers.Dense(128,activation='relu'), # second layer consists of 128 neurons\n",
    "                   layers.Dense(len_of_class) # last layer should consists of number of neurons=number of classes\n",
    "                   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c9fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T16:15:26.663621Z",
     "start_time": "2022-09-28T16:15:26.645625Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57690dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:30:58.337408Z",
     "start_time": "2022-10-31T17:30:57.522412Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd002fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T16:19:41.578015Z",
     "start_time": "2022-09-28T16:19:04.044538Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "history = model.fit(x=x_train,y=y_train,batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eafa76c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T16:16:21.600875Z",
     "start_time": "2022-09-28T16:16:21.594882Z"
    }
   },
   "outputs": [],
   "source": [
    "## Test Your Model with the Test Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb659f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T17:13:45.275368Z",
     "start_time": "2022-09-28T17:13:44.929344Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(3,2,figsize=(10,8))\n",
    "for image_test,label_test, ax in zip(x_test,y_test,axs.ravel()):\n",
    "    y_pred=np.argmax(model.predict(image_test.reshape(1,-1))) # returns the position for given maximum probability \n",
    "                                                              # reshape(1,-1) is similar to flattening , flatten a tensor to an \n",
    "                                                              # array of 1 row and N columns.\n",
    "                                                              # use model.predict to get predicted labels value for given test image\n",
    "    ax.imshow(image_test)\n",
    "    ax.set_title(f'actual image:{label_test}, predicted image :{y_pred}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318f53c",
   "metadata": {},
   "source": [
    "# Exercise for Students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4571ca",
   "metadata": {},
   "source": [
    "In this exercise, students are going to build a MLP model on a dummy dataset . Some of the parts in the code has been already done, students are suppose to fill up the remaining scripts according to some instructions. You may refer to the code provided in the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22af7146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:31:30.692007Z",
     "start_time": "2022-10-31T17:31:30.085007Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53edc4be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:31:30.708010Z",
     "start_time": "2022-10-31T17:31:30.693007Z"
    }
   },
   "outputs": [],
   "source": [
    "# let us create a dummy dataset \n",
    "X=np.random.randn(100,1)\n",
    "y=np.sin(X*2*np.pi/3)\n",
    "# let us add some noise to the target \n",
    "y_hat=y+0.2*np.random.randn(y.shape[0]).reshape(y.shape[0],1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead17c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:31:31.336306Z",
     "start_time": "2022-10-31T17:31:31.324269Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(np.c_[X.reshape(-1,),y_hat.reshape(-1,)], columns=['X','y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "694005cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:15:58.907815Z",
     "start_time": "2022-10-31T18:15:58.890818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.303765</td>\n",
       "      <td>0.968790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.381042</td>\n",
       "      <td>-0.776168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.376401</td>\n",
       "      <td>-1.073799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294470</td>\n",
       "      <td>0.426786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.333954</td>\n",
       "      <td>-0.401633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.359093</td>\n",
       "      <td>-0.565931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.546444</td>\n",
       "      <td>-0.434908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.305958</td>\n",
       "      <td>-0.809907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.294179</td>\n",
       "      <td>0.345610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.323364</td>\n",
       "      <td>-0.776146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X     y_hat\n",
       "0   0.303765  0.968790\n",
       "1  -0.381042 -0.776168\n",
       "2  -0.376401 -1.073799\n",
       "3   0.294470  0.426786\n",
       "4  -1.333954 -0.401633\n",
       "..       ...       ...\n",
       "95 -0.359093 -0.565931\n",
       "96  1.546444 -0.434908\n",
       "97 -0.305958 -0.809907\n",
       "98  1.294179  0.345610\n",
       "99 -0.323364 -0.776146\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedc4a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:31:32.089523Z",
     "start_time": "2022-10-31T17:31:31.905528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x193565c7a60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "deb5fe2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:22:03.975534Z",
     "start_time": "2022-10-31T18:22:03.969538Z"
    }
   },
   "outputs": [],
   "source": [
    "## TO DO \n",
    "## USE train_test_split to split data set (X and y_hat) in train ,val_test (train=80% and test=20%)\n",
    "## again use val_test  data and split into 90% val and 10% test\n",
    "## YOUR CODE HERE\n",
    "## use train_test_split from sklearn and X=df.X and y=df.y\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(df.values[:,0],df.values[:,1],test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f16a797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:16:42.111417Z",
     "start_time": "2022-10-31T18:16:42.101419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is :(80,)\n",
      "The shape of X_val is :(18,)\n",
      "The shape of y_train is :(80,)\n",
      "The shape of y_val is :(18,)\n",
      "The shape of X_test is :(2,)\n",
      "The shape of y_test is :(2,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of X_train is :{X_train.shape}') ## train data\n",
    "print(f'The shape of X_val is :{X_val.shape}') ## validation data \n",
    "print(f'The shape of y_train is :{y_train.shape}') ##  validation labels \n",
    "print(f'The shape of y_val is :{y_val.shape}')## test labels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f341df2",
   "metadata": {},
   "source": [
    "BUILD A MODEL WITH GIVEN SPECIFICATION\n",
    "Layers               Neurons              Activation Function \n",
    "Dense Layer-I        10                   Relu\n",
    "Dense Layer-II       11                   Relu \n",
    "Dense Layer-III       1                     -\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Layers</th>\n",
    "      <th>Neurons</th>\n",
    "      <th>Activation Function</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Dense Layer-I</td>\n",
    "      <td>10</td>\n",
    "      <td>ReLU</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "     <td>Dense Layer-II</td>\n",
    "      <td>11</td>\n",
    "      <td>ReLU</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "     <td>Dense Layer-II</td>\n",
    "      <td>1</td>\n",
    "      <td>-</td>\n",
    "    </tr> \n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04e68ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:16:44.797594Z",
     "start_time": "2022-10-31T18:16:44.759591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Refer https://keras.io/api/\n",
    "model=tf.keras.models.Sequential()\n",
    "## TO DO \n",
    "## YOUR CODE HERE\n",
    "model.add(tf.keras.Input(shape=(1,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(11, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))  # remember number of neurons in last layer is \n",
    "                                                        # equal to number of output variable that is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ab84bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:20:26.138621Z",
     "start_time": "2022-10-31T18:20:26.119616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 11)                121       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bec74fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:18:40.663308Z",
     "start_time": "2022-10-31T18:18:40.648307Z"
    }
   },
   "outputs": [],
   "source": [
    "##TO DO \n",
    "## Use optimizer 'adam' and loss as 'mean_squared_error'\n",
    "# YOu may use metrics as  tf.keras.metrics.MeanSquaredError()\n",
    "##YOUR CODE HERE \n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),metrics=tf.keras.metrics.MeanSquaredError(),\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4666a620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:21:15.926946Z",
     "start_time": "2022-10-31T18:20:57.152296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "80/80 [==============================] - 0s 937us/step - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 2/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 3/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 4/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 5/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 6/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 7/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "Epoch 8/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 9/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 10/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0495 - mean_squared_error: 0.0495 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 11/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 12/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 13/300\n",
      "80/80 [==============================] - 0s 718us/step - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 14/300\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 15/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 16/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 17/300\n",
      "80/80 [==============================] - 0s 797us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "Epoch 18/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 19/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 20/300\n",
      "80/80 [==============================] - 0s 720us/step - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 21/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0569 - mean_squared_error: 0.0569 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 22/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 23/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 24/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 25/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 26/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 27/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 28/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 29/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0699 - val_mean_squared_error: 0.0699\n",
      "Epoch 30/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 31/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0607 - val_mean_squared_error: 0.0607\n",
      "Epoch 32/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 33/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 34/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0474 - mean_squared_error: 0.0474 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 35/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0644 - val_mean_squared_error: 0.0644\n",
      "Epoch 36/300\n",
      "80/80 [==============================] - 0s 797us/step - loss: 0.0562 - mean_squared_error: 0.0562 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 37/300\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 38/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 39/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 40/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 41/300\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 42/300\n",
      "80/80 [==============================] - 0s 797us/step - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 43/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 44/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 45/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 46/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 47/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 48/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 49/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 50/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 51/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 747us/step - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 52/300\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0929 - val_mean_squared_error: 0.0929\n",
      "Epoch 53/300\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 54/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0674 - val_mean_squared_error: 0.0674\n",
      "Epoch 55/300\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 56/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 57/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 58/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 59/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 60/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0502 - mean_squared_error: 0.0502 - val_loss: 0.0640 - val_mean_squared_error: 0.0640\n",
      "Epoch 61/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 62/300\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 63/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0659 - val_mean_squared_error: 0.0659\n",
      "Epoch 64/300\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 65/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 66/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 67/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 68/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 69/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 70/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 71/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 72/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 73/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 74/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 75/300\n",
      "80/80 [==============================] - 0s 668us/step - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 76/300\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 77/300\n",
      "80/80 [==============================] - 0s 725us/step - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 78/300\n",
      "80/80 [==============================] - 0s 694us/step - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 79/300\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 80/300\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 81/300\n",
      "80/80 [==============================] - 0s 738us/step - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 82/300\n",
      "80/80 [==============================] - 0s 753us/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 83/300\n",
      "80/80 [==============================] - 0s 774us/step - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 84/300\n",
      "80/80 [==============================] - 0s 761us/step - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 85/300\n",
      "80/80 [==============================] - 0s 741us/step - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0583 - val_mean_squared_error: 0.0583\n",
      "Epoch 86/300\n",
      "80/80 [==============================] - 0s 937us/step - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 87/300\n",
      "80/80 [==============================] - 0s 766us/step - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 88/300\n",
      "80/80 [==============================] - 0s 739us/step - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 89/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 90/300\n",
      "80/80 [==============================] - 0s 940us/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 91/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 92/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 93/300\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "Epoch 94/300\n",
      "80/80 [==============================] - 0s 758us/step - loss: 0.0535 - mean_squared_error: 0.0535 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 95/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 96/300\n",
      "80/80 [==============================] - 0s 732us/step - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0597 - val_mean_squared_error: 0.0597\n",
      "Epoch 97/300\n",
      "80/80 [==============================] - 0s 692us/step - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 98/300\n",
      "80/80 [==============================] - 0s 768us/step - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 99/300\n",
      "80/80 [==============================] - 0s 724us/step - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 100/300\n",
      "80/80 [==============================] - 0s 715us/step - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 776us/step - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 102/300\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 103/300\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 104/300\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 105/300\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 106/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0594 - val_mean_squared_error: 0.0594\n",
      "Epoch 107/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 108/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 109/300\n",
      "80/80 [==============================] - 0s 704us/step - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 110/300\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 111/300\n",
      "80/80 [==============================] - 0s 766us/step - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 112/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 113/300\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 114/300\n",
      "80/80 [==============================] - 0s 745us/step - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 115/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 116/300\n",
      "80/80 [==============================] - 0s 743us/step - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 117/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 118/300\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 119/300\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 120/300\n",
      "80/80 [==============================] - 0s 962us/step - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 121/300\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 122/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 123/300\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 124/300\n",
      "80/80 [==============================] - 0s 787us/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 125/300\n",
      "80/80 [==============================] - 0s 925us/step - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 126/300\n",
      "80/80 [==============================] - 0s 735us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 127/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0667 - val_mean_squared_error: 0.0667\n",
      "Epoch 128/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 129/300\n",
      "80/80 [==============================] - 0s 727us/step - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 130/300\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 131/300\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 132/300\n",
      "80/80 [==============================] - 0s 719us/step - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 133/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 134/300\n",
      "80/80 [==============================] - 0s 703us/step - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 135/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 136/300\n",
      "80/80 [==============================] - 0s 783us/step - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 137/300\n",
      "80/80 [==============================] - 0s 718us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 138/300\n",
      "80/80 [==============================] - 0s 679us/step - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 139/300\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 140/300\n",
      "80/80 [==============================] - 0s 715us/step - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "Epoch 141/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 142/300\n",
      "80/80 [==============================] - 0s 762us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 143/300\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 144/300\n",
      "80/80 [==============================] - 0s 750us/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 145/300\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 146/300\n",
      "80/80 [==============================] - 0s 688us/step - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 147/300\n",
      "80/80 [==============================] - 0s 724us/step - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "Epoch 148/300\n",
      "80/80 [==============================] - 0s 756us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 149/300\n",
      "80/80 [==============================] - 0s 731us/step - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 150/300\n",
      "80/80 [==============================] - 0s 724us/step - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/300\n",
      "80/80 [==============================] - 0s 744us/step - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 152/300\n",
      "80/80 [==============================] - 0s 687us/step - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0594 - val_mean_squared_error: 0.0594\n",
      "Epoch 153/300\n",
      "80/80 [==============================] - 0s 851us/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 154/300\n",
      "80/80 [==============================] - 0s 822us/step - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 155/300\n",
      "80/80 [==============================] - 0s 751us/step - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 156/300\n",
      "80/80 [==============================] - 0s 749us/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 157/300\n",
      "80/80 [==============================] - 0s 714us/step - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 158/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 159/300\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 160/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 161/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 162/300\n",
      "80/80 [==============================] - 0s 718us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 163/300\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 164/300\n",
      "80/80 [==============================] - 0s 776us/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 165/300\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 166/300\n",
      "80/80 [==============================] - 0s 772us/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 167/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 168/300\n",
      "80/80 [==============================] - 0s 687us/step - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 169/300\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 170/300\n",
      "80/80 [==============================] - 0s 801us/step - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 171/300\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 172/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 173/300\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 174/300\n",
      "80/80 [==============================] - 0s 707us/step - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0652 - val_mean_squared_error: 0.0652\n",
      "Epoch 175/300\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.0479 - mean_squared_error: 0.0479 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 176/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 177/300\n",
      "80/80 [==============================] - 0s 710us/step - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 178/300\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 179/300\n",
      "80/80 [==============================] - 0s 693us/step - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 180/300\n",
      "80/80 [==============================] - 0s 728us/step - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 181/300\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 182/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 183/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 184/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 185/300\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 186/300\n",
      "80/80 [==============================] - 0s 700us/step - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 187/300\n",
      "80/80 [==============================] - 0s 766us/step - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 188/300\n",
      "80/80 [==============================] - 0s 702us/step - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 189/300\n",
      "80/80 [==============================] - 0s 801us/step - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 190/300\n",
      "80/80 [==============================] - 0s 674us/step - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 191/300\n",
      "80/80 [==============================] - 0s 692us/step - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 192/300\n",
      "80/80 [==============================] - 0s 688us/step - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 193/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 194/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 195/300\n",
      "80/80 [==============================] - 0s 690us/step - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 196/300\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 197/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 198/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "Epoch 199/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 200/300\n",
      "80/80 [==============================] - 0s 705us/step - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/300\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 202/300\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0638 - val_mean_squared_error: 0.0638\n",
      "Epoch 203/300\n",
      "80/80 [==============================] - 0s 717us/step - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 204/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 205/300\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 206/300\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 207/300\n",
      "80/80 [==============================] - 0s 739us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 208/300\n",
      "80/80 [==============================] - 0s 712us/step - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 209/300\n",
      "80/80 [==============================] - 0s 716us/step - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 210/300\n",
      "80/80 [==============================] - 0s 689us/step - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 211/300\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 212/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 213/300\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 214/300\n",
      "80/80 [==============================] - 0s 679us/step - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0634 - val_mean_squared_error: 0.0634\n",
      "Epoch 215/300\n",
      "80/80 [==============================] - 0s 680us/step - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 216/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 217/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 218/300\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 219/300\n",
      "80/80 [==============================] - 0s 962us/step - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 220/300\n",
      "80/80 [==============================] - 0s 924us/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 221/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 222/300\n",
      "80/80 [==============================] - 0s 937us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 223/300\n",
      "80/80 [==============================] - 0s 962us/step - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 224/300\n",
      "80/80 [==============================] - 0s 924us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 225/300\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0607 - val_mean_squared_error: 0.0607\n",
      "Epoch 226/300\n",
      "80/80 [==============================] - 0s 949us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 227/300\n",
      "80/80 [==============================] - 0s 700us/step - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 228/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0647 - val_mean_squared_error: 0.0647\n",
      "Epoch 229/300\n",
      "80/80 [==============================] - 0s 853us/step - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 230/300\n",
      "80/80 [==============================] - 0s 699us/step - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 231/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 232/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 233/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 234/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 235/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 236/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 237/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 238/300\n",
      "80/80 [==============================] - 0s 937us/step - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 239/300\n",
      "80/80 [==============================] - 0s 1000us/step - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 240/300\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 241/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0697 - val_mean_squared_error: 0.0697\n",
      "Epoch 242/300\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 243/300\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0853 - val_mean_squared_error: 0.0853\n",
      "Epoch 244/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 245/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 246/300\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.0435 - mean_squared_error: 0.0435 - val_loss: 0.0624 - val_mean_squared_error: 0.0624\n",
      "Epoch 247/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 248/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0649 - val_mean_squared_error: 0.0649\n",
      "Epoch 249/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0433 - mean_squared_error: 0.0433 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 250/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 252/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 253/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 254/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 255/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 256/300\n",
      "80/80 [==============================] - 0s 709us/step - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 257/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 258/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 259/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 260/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 261/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 262/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "Epoch 263/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 264/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 265/300\n",
      "80/80 [==============================] - 0s 721us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 266/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 267/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 268/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 269/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0649 - val_mean_squared_error: 0.0649\n",
      "Epoch 270/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 271/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0576 - val_mean_squared_error: 0.0576\n",
      "Epoch 272/300\n",
      "80/80 [==============================] - 0s 671us/step - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 273/300\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 274/300\n",
      "80/80 [==============================] - 0s 684us/step - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 275/300\n",
      "80/80 [==============================] - 0s 722us/step - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 276/300\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 277/300\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 278/300\n",
      "80/80 [==============================] - 0s 734us/step - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 279/300\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0599 - val_mean_squared_error: 0.0599\n",
      "Epoch 280/300\n",
      "80/80 [==============================] - 0s 831us/step - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 281/300\n",
      "80/80 [==============================] - 0s 683us/step - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 282/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 283/300\n",
      "80/80 [==============================] - 0s 772us/step - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 284/300\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 285/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 286/300\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 287/300\n",
      "80/80 [==============================] - 0s 747us/step - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 288/300\n",
      "80/80 [==============================] - 0s 759us/step - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 289/300\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 290/300\n",
      "80/80 [==============================] - 0s 772us/step - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0838 - val_mean_squared_error: 0.0838\n",
      "Epoch 291/300\n",
      "80/80 [==============================] - 0s 772us/step - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0616 - val_mean_squared_error: 0.0616\n",
      "Epoch 292/300\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 293/300\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 294/300\n",
      "80/80 [==============================] - 0s 899us/step - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 295/300\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 296/300\n",
      "80/80 [==============================] - 0s 962us/step - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 297/300\n",
      "80/80 [==============================] - 0s 949us/step - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0601 - val_mean_squared_error: 0.0601\n",
      "Epoch 298/300\n",
      "80/80 [==============================] - 0s 924us/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 299/300\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0586 - val_mean_squared_error: 0.0586\n",
      "Epoch 300/300\n",
      "80/80 [==============================] - 0s 886us/step - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x193673e6a00>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=300\n",
    "## USe epoch=10,and fit your model in here \n",
    "# Report your accuracy in Quiz\n",
    "model.fit(x=X_train,y=y_train,batch_size=1,validation_data=(X_val,y_val),epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cdab2502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T18:22:08.849989Z",
     "start_time": "2022-10-31T18:22:08.702004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x19367413070>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Make prediction for X \n",
    "## YOUR CODE HERE\n",
    "plt.scatter(X,model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f5779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "749px",
    "left": "34px",
    "top": "111.125px",
    "width": "345.938px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
