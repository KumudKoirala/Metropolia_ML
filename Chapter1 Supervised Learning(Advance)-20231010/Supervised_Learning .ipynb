{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44088bd2",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd635eb",
   "metadata": {},
   "source": [
    "A decision treee is a hierarchical, non parametric model for supervised learning whereby the local region is identified in a sequence of recursive splits in a smaller number of steps. A decision tree is composed of internal decision nodes and terminal leaf nodes(Figure 1.1).\n",
    "Decision Tree Decision Algorithm starts from the tree node(root) and split the data on the feature that results in the largest Information Gain(IG).Based on the features of our training dataset, Decision Trees model learns a series of questions to infer the class labels of the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28531fc",
   "metadata": {},
   "source": [
    "![figure_1_1.png](images/figure_1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42646d89",
   "metadata": {},
   "source": [
    "Decision trees are tree like graph that models a question. An analogy for Decision trees is \"twenty questions\" where the player has to make guess about the object by saying out some specific names which could be related to the object. Then the opponent replies with 'yes' or 'no'. Based on the features(names)and opponent 'yes' or 'no' , a player tries to model the object and give a specific name of object at the end and wins the game if the player correctly predicts the object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e20f4",
   "metadata": {},
   "source": [
    "The interior nodes of decision trees test features.The training instances are divided into the subsets based on the outcomes of the test.For example,in figure 1.1 a node may test a variable 'outlook' is  'sunny' or not and if yes then the outcome would be 'Go to Beach'. These tests are carried out until a stopping criteria is reached. When a decision tree has been created, making a prediction for test data requires following the edges until a leaf node is reached.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5931d0",
   "metadata": {},
   "source": [
    "![figure_1_4.png](images/figure_1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d7b66",
   "metadata": {},
   "source": [
    "Let us look at a dataset given above where we have to categorize to animal species cat and dog based on their features which are plays_fetch, grumpy and favourite food. Play_fetch and grumpy have Yes/No values where as favourite food has Bacon, Dog Food and cat food. Decision tree examines features at each node. A first node would ask if an animal plays fetch or not. If the answer is yes, we will follow  the edge that leads to the left internal node otherwise to the right internal node. As we keep on questioning the featured values, based on some answer we will end to leaf node which will decide if the species is cat or Dog. \n",
    "\n",
    "From the above dataset, we can see that most dogs love to play fetch and are less grumpier than cat. Dogs love dog food and bacon whereas cat loves catfood and bacon. We can easily convert the categorical variable in the given dataset which encoders(Beginner module chapter 4).These rules can be very much difficult to make using pen and paper and therefore we will using python scikit learn libraries. \n",
    "\n",
    "In the upcoming section, we are going to build a tree for the above given dataset using an algorithm called Iterative Dichotomiser3(ID3). ID3 algorithm sets a feature as root node and splits the selected feature to produce the subsets of the data.For each subset, it calculates the entropy or Information gain and selects the particular output which results in maximum Information Gain(IG). The algorithm performs the similar operation on all the features which was not selected before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce0173",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a7683",
   "metadata": {},
   "source": [
    "Now we know that decision tree test the feature values by making questions, but still we are unknown what features should be tested first. Let us look at two different scatterplots for two different dataset in the figure below. The scatterplot consist of two features: feature 1 and feature 2. Both of the dataset consists of two classes:red and green. A dotted line known as decision boundary splits the dataset so that the region separated by the boundary consists with maximum data points of the particular class. If we observe the left hand side plot, the blue dotted line has perfectly splitted the red and green class. What kind of feature test question could be made from this plot ? If feature1<2.5 , class =red otherwise class=green.So we donot need feature 2 to infer about the threshold.\n",
    "\n",
    "However if we look at the scatterplot on the right hand side, some datapoints from both classes have almost overlapped. So which one would be the best split. If you assume the blue one is the best split , then you have perfect region for green class but some red class sample points are present too in the green classes. If we choose the orange has a decision boundary then some green class samples has showed up in the red class region . \n",
    "\n",
    "To resolve the issue above we take the help of Entropy. Entropy can be use to quantify the uncertainty.The test that reduces our uncertainty about the classification are the best.Entropy are measured in bits and  can be expressed as\n",
    "$$\\text{H(X)} = -\\sum_{i=1}^n P(X_{i})\\log_{2} P(X_{i})  $$\n",
    "where n is the number of outcome\n",
    " and $P(X_{i})$ is the probability of outcome i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246826b",
   "metadata": {},
   "source": [
    "![figure_1_5.png](images/figure_1_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e774f3a",
   "metadata": {},
   "source": [
    "As an example, lets us take a fair coin and toss it in the air. The probability of getting head is 0.5 and also probability of getting tail is 0.5. The entropy of coin toss is \n",
    "\n",
    "$H(X)=-(0.5log_{2}0.5+0.5log_{2}0.5)=1$\n",
    "\n",
    "So only 1 bit is required to encode the outcome which could be 1=head 0= tail or the otherway round. So we are certain that there will be two outcomes.However if we take a biased coin with $P(H)=0.8$ and $P(T)=0.2$, the entropy for coin toss is \n",
    "\n",
    "$H(X)=-(0.8log_{2}0.8+0.2log_{2}0.2)=0.72$\n",
    "\n",
    "The entropy is a fraction of 1 bit which still has two outcomes head and tail, however head occuring most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365bf7f",
   "metadata": {},
   "source": [
    "let us the take the dataset of cat and dog classification problem in the example above. Our training dataset consists of unequal number of classes with 6 dogs and 8 cats. If we just the number of animals present in the dataset and nothing else, just like the coin toss, we can calculate the entropy of decision/choice as \n",
    "\n",
    "$H(X)=-(\\frac{6}{14}log_{2}\\frac{6}{14}+\\frac{8}{14}log_{2}\\frac{8}{14})=0.9852$.\n",
    "\n",
    "So we are still uncertain about the outcome because the amount of cats are higher than the dog. \n",
    "\n",
    "Now we would use the feature information to classify the animals from the dataset and split the data based on the animal that  plays fetch and which doesnot. So we have 9 samples that doesnot play fetch and 5 samples that play fetch. In 9 samples, 7 samples belongs to class Dog and 2 sample belonging to class Cat. Out of  5 samples which play fetch, 4 samples belongs to Dog and 1 sample belonging to class Cat.\n",
    "The entropy at the node leading with play fetch=No  is \n",
    "\n",
    "$H(X)=-(\\frac{2}{9}log_{2}\\frac{2}{9}+\\frac{7}{9}log_{2}\\frac{7}{9})=0.76$.\n",
    "\n",
    "The  entropy at the node leading with play fetch=Yes is \n",
    "\n",
    "$H(X)=-(\\frac{1}{5}log_{2}\\frac{1}{5}+\\frac{4}{5}log_{2}\\frac{4}{5})=0.72$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc37274",
   "metadata": {},
   "source": [
    "Similary you proceed with the same manner to test other two features. I am not going to present the calculation steps for other two features but if you want to do it please free to do it by yourself. The chart below contains the entropy result from each of the test features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f8bb8",
   "metadata": {},
   "source": [
    "![figure_1_6.png](images/figure_1_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ce65a",
   "metadata": {},
   "source": [
    "## Information Gain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb93e89",
   "metadata": {},
   "source": [
    "Based on the different feature split on the dataset , we would  have different entropy for each split. Now the next question arises, which of these splits reduces out uncertainty. If we average the entropy across the different feature, the subset produced by the test question favourite food gives the lowest average entropy=$\\frac{1}{2}(0.8113+0)$. However this way of calculating the average of entropy across the feature and taking the minimum might not result in optimal tree.\n",
    "\n",
    "We know that higher the entropy, the more mixed up the data is and therefore we have to  reorganize the data so as to minimize the entropy.Reorganizing was done based on the feature split.When we splited the dataset on the features there was also change in the information. The change in the information before and after the split is known as the information Gain(IG).\n",
    "\n",
    "The information gain(IG) can be expressed as:\n",
    "$$IG(T,a)=H(T)-\\sum_{v\\in vals(a)} \\frac{|{x \\in T|x_a=v}|}{|T|}H({x \\in T|x_a=v}) $$\n",
    "\n",
    "$H(T)$ is the entropy of parent node\n",
    "\n",
    "$x_a \\in vals(a) $ is the value of a feature a for instance x\n",
    "\n",
    "${x \\in T|x_a=v}$ is the number of instances for which feature $a$ is equal to value $v$\n",
    "\n",
    "$H({x \\in T|x_a=v})$ is the subset of instances for which the value of feature $a$ is $v$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d54c66",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\n",
    "& \\text {Table1.1: Information Gain(IG) calculated on the different subset of dataset obtained by splitting on features }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline \\text { Test } & \\text {Parent's Entropy } & \\text {Left-child Entropy} &  \\text{Right-child Entropy} &  \\text{Weighted Average} &  \\text{IG} \\\\\n",
    "\\hline playfetch? & 0.9852 & 0.7642 & 0.7219 & 0.7642*\\frac{9}{14}+0.7219*\\frac{5}{14}=0.7491 & 0.2361  \\\\\n",
    "grumpy? & 0.9852 & 0.9183 & 0.8113 & 0.9183*\\frac{6}{14}+0.8113*\\frac{8}{14}=0.857 & 0.1280 \\\\\n",
    "cat food?& 0.9852 & 0.8113 & 0 & 0.8113*\\frac{8}{14}+0*\\frac{6}{14}=0.4636 & 0.5216\\\\\n",
    "dog food? & 0.9852 & 0.8454& 0 & 0*\\frac{11}{14}+0*\\frac{3}{14}=0.6642 & 0.3210\\\\\n",
    "bacon?& 0.9852 & 0.9183 & 0.971&0.9183*\\frac{9}{14}+0.971*\\frac{5}{14}=0.9371 & 0.0481\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e666d8",
   "metadata": {},
   "source": [
    "The table below above shows weighted average and  the Information Gain(IG) for each of the subset produced for cat and dog classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48bc0e0",
   "metadata": {},
   "source": [
    "From table above, we know that the test feature cat food has the highest Information Gain(IG). So we select the cat food as our parent/root node of the decision tree. However if we observe the leaf node , we still dont have the node which is pure. Pure leaf is the leaf of the tree that contains datapoints that all share the same target value. In the left child node for subset cat food , we can see there are 2 samples belonging to class dog and 6 samples belonging to the class cat. However the right child node in the same subset has the pure leaf because all of the 6 samples belongs to class Dog and doesnot contain any sample from the cat. In order to get the pure leaf from the left child node, we again split the left child node and apply ID3 algorithm. This time we will use this 8 samples to test with other untested features and calculate the entropy and Information Gain. As we keep on splitting the child node ,test the subset on different features and select the one test feature  which has highest IG , this process increases the depth of Decision Tree.As we keep on repeating the same procedure, we will end up with complete the decision tree shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26948b85",
   "metadata": {},
   "source": [
    "![figure_1_7.png](images/figure_1_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f338ff",
   "metadata": {},
   "source": [
    "## Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d75d",
   "metadata": {},
   "source": [
    "We can split the dataset based on features using the best Information Gain(IG) and create a decision tree. There is another heurestic approach for making learning decision which is known as Gini Impurity.Gini Impurity measures the proportions of classes in a set. There is no any rules or criteria when to use IG or the Gini Impurity , in practice, they tend to give results that differ by small values.\n",
    "Gini Impurity can be expressed as\n",
    "\n",
    "$$Gini(t)=1-\\sum_{i=1}^jP(i \\mid t)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8acfb2",
   "metadata": {},
   "source": [
    "Gini Impurity is highest when each class has an equal probability of being selected and lowest when all the elements of the set belongs to the same class.Python's library scikit uses Gini Impurity by default but we can choose Information Gain as metric in scikit-learn library to build the decision tree. We are going to take an same dog and cat  example, to test the feature  favourite food= cat food?. The gini impurity for left child node and right child node can be written as \n",
    "\n",
    "$Gini(t)_{\\text{LCN}}=1- (\\frac{2}{8})^2-(\\frac{6}{8})^2 =0.375$\n",
    "\n",
    "$Gini(t)_{\\text{RCN}}=1- (\\frac{6}{6})^2-(\\frac{0}{6})^2=0$\n",
    "\n",
    "weighted average Gini Impurity for the cat food? test feature =$ 0.375*\\frac{8}{14}+0*\\frac{6}{14}=0.214$\n",
    "\n",
    "We take the split/subset which gives the lowest weighted average gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76513546",
   "metadata": {},
   "source": [
    "# Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07a718",
   "metadata": {},
   "source": [
    "In the beginner module, we have evaluated classification problem using accuracy which was the fraction of correctly classified samples. However measuring accuracy based on the fraction of correct sample might not be a good idea when it comes to unbalanced classes and it is important to choose the right metric for your ML application.\n",
    "\n",
    "Binary classification has the two outcomes. One way of representing the outcomes could be positive and negative and the other way could be 1/0. Imagine a medical test which results in positive or negative result. However the situation could be different if the test has been tempered. This could lead to a different actual result known as False positive(FP) and False Negative(FN). A healthy patient could be Falsely identified as Positive and  sick patient could be identified as Falsely identified as  Negative. True Positive(TP) and True Negative(TN) is something we desired to be higher so  we can assure patient that the test works perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e2d7",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f27682",
   "metadata": {},
   "source": [
    "Confusion matrix are the ways to represent the result of evalutating binary classification. For a binary classification problem, the confusion matrix is of $2\\times2$ order. Each row of confusion matrix corresponds to true classes and the columns corresponds to predicted classes. The figure below shows an example of confusion matrix where the task is the classify the dog and cat species. Each element in the matrix  are the count values of identified species. For example out of 150 dog samples, only 120 were truely identified as a dog and 30 were  falsely identified as cat. We can categorized dog as a positive class and cat as negative class or vice versa because they are exchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d36d5f",
   "metadata": {},
   "source": [
    "![figure_1_8.png](images/figure_1_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb86744",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f404a7",
   "metadata": {},
   "source": [
    "Accuracy can be defined as number of accurate predictions(TP+TN) divided by the total number of samples used for classification. Mathematically, it can be expressed as\n",
    "\n",
    "$$\\text{Accuracy}= \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060d3f8",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa455db3",
   "metadata": {},
   "source": [
    "Precision measures how many of the samples predicted as positive are truelly positive. We use precision as our metric if we want to limit the number of false positive. Let us take an example where we want to test the result of new introduce drug in the general population. Therefore it is important that drug should not produce many False positive so that the company can safely release its new drug for general public. The precision can be calculated as shown below\n",
    "\n",
    "$$\\text{precision} =\\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e8fe0",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a550cd",
   "metadata": {},
   "source": [
    "Recall measures how many positive samples are captured by the positive predictions and can be expressed as\n",
    "\n",
    "$$\\text{recall}=\\frac{TP}{TP+FN} $$\n",
    "\n",
    "Recall metric is important when it becomes necessary to avoid false negative. For example in the case of new cancer diagnosis, it is important to find all the  person who has cancer and also some healthy people who could be falsely identified as a cancer patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bb6a0",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd3c29",
   "metadata": {},
   "source": [
    "Both recall and precision are the important metrics use for different problem statement. One way to summarize both precision and recall is F1 score which takes both of them into account and can measure better accuracy on imbalanced binary classification. F1 can be expressed as harmonic mean of precision and recall \n",
    "$$F=2\\frac{precision\\cdot call}{precision+call}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68e4c4",
   "metadata": {},
   "source": [
    "# Underfit , Goodfit And Overfit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87eee3",
   "metadata": {},
   "source": [
    "Overfitting and Underfitting are two main problems when machine learning model has been deployed. In this section we are going to learn these two problems that occurs commonly in ML deployment .\n",
    "Overfitting occurs when a model performs well on a training data but performs poorly in a test(unseen) dataset. When a model is said to have a \"High Variance\" and \"Lower bias\" when it overfits the dataset. Regularization method is one of the way to tackle the Overfitting situation. \n",
    "Underfitting occurs when our model is not complex enough to capture the correct pattern in training data and also performs poorly with the test or unseen data set . An underfit model has a higher bias and a lower variance.\n",
    "A good fit model performs well with both training and test dataset and has a good tradeoff with bias and variance. \n",
    "![figure_1_3.png](images/figure_1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739abca",
   "metadata": {},
   "source": [
    "##  Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bc072",
   "metadata": {},
   "source": [
    "In order to evaluate the prformace of a ML model on a given data set, we need some way to measure how well prediction actually match the observed data. Therefore, we need to quantify and compare the predicted response value for a given dataset with the true response value for the particular observation. In the regression problem, the most commonly used measure is the mean squared error(also known as loss function) given as \n",
    "$$MSE =  J{(w)} = \\frac{1}{n}  \\sum_{i=1}^{n}  (\\hat{y}_i -y_i)^2                     $$\n",
    "\n",
    "where, $\\hat{y}$   is the predicted value and $y$ is the true value.\n",
    "\n",
    "The MSE will be small if the predicted responses are very close to the true responses and will be large for some observations if the predicted and true values differ substantially. These are mostly covered in the <b>beginner level of this course.</b>\n",
    "The MSE is computed using the training data that has been used to fit the model and can be called as <i>training MSE.</i>However, we are interested in the accuracy of the predictions that we obtain when we apply our trained ML model to unseen test data. \n",
    "Linear Regression finds the optimal parameters by minimizing the mean square error however linear regression model doesnot have additional parameters to control the model complexity.\n",
    "\n",
    "Researchers uses the term \"bias\" and \"variance\" to describe the performance of a model. In ML models, variance measures the consistency/variability of the model prediction for classifying a particular example if we retrain the model multiple times on different batches of the training dataset. ML Model could be less or highly sensitive to the randomness in the training data.\n",
    "Bias measures how far off the predictions are from the correct values in general if we train the model multiple times on different batches of training sets. Bias is the measure of the systematic error that does not occur due to randomness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa9e0d",
   "metadata": {},
   "source": [
    "# Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0235486",
   "metadata": {},
   "source": [
    "In the first module(beginners) of ML , we used a linear regression assuming that there is a linear relationship between independent(explanatory/features) and dependent variables(target/response variable). However there could be a non-linear relationship between the explanatory and response variable. In figure 1.3(first from left ), we can see that a straighline fit(a simple model) actually causes the underfit situation. Our desire would be to make the model more complex so that it can overcome the underfit situation introduced by a simple linear model. \n",
    "\n",
    "Polynomial features helps to create new features in the dataset which can help to turn our ML model more complex. For example if we have features $[x_1,x_2,x_3]$ in our original dataset the resultant polynomial features of <b>degree=2</b> would be $[1,x_1,x_2, x_3, x^2_1, x^2_2, x^2_3, x_1x_2, x_1x_3, x_2x_3]$. The degree of the polynomial features control the number of features created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37837f",
   "metadata": {},
   "source": [
    "## Example : Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([[1,2],[2,3],[5,6]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fit=PolynomialFeatures(degree=2).fit_transform(data)\n",
    "print(data_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745d11c",
   "metadata": {},
   "source": [
    "Now if we compare the two matrices data and data_fit, we can easily see the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c522345",
   "metadata": {},
   "source": [
    "# Regularization Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8c616",
   "metadata": {},
   "source": [
    "When a model tends to be highly complex(Figure 1.3 third from left),model becomes too good for the training data and has a higher accuracy for the training data. However with the test data , model might not work effectively. In this section we are going to look at the two regularizing technique used in Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f329668",
   "metadata": {},
   "source": [
    "## L2 Regularization Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfc035",
   "metadata": {},
   "source": [
    "A regression model that uses L2 regularization technique is called <i>Ridge Regression</i>. Ridge regression is an L2 penalized model where we simply add the swquare sum of the weights to put least-squares costfunction.\n",
    "$$ J{(w)} = \\frac{1}{n}  \\sum_{i=1}^{n}  (\\hat{y}_i -y_i)^2     +  \\lambda ||w||_2^2              $$  \n",
    "$$ L2: \\lambda ||w||_2^2 = \\lambda   \\sum_{j=1}^{m} w_j^2             $$\n",
    "\n",
    "When the hyperparameter $\\lambda$ increases, we increase the strength of regularization and thereby shrink the weights of our models. We do not regularize the bias or intercept term ($w_0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9589ea8",
   "metadata": {},
   "source": [
    "##  L1  Regularization Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3677bdb",
   "metadata": {},
   "source": [
    "A regression model that uses L1 regularization technique is called <i>Lasso Regression</i>. Lasso shrinks the less important features coefficient to 0 , thus reducing the features in the dataset.Lasso regression works for the feature selection aswell where we are suppose to choose only the important features in our dataset.\n",
    "$$ J{(w)} = \\frac{1}{n}  \\sum_{i=1}^{n}  (\\hat{y}_i -y_i)^2     +  \\lambda ||\\textbf {w}||_1             $$  \n",
    "$$ L1: \\lambda ||\\textbf {w}||_1 = \\lambda   \\sum_{j=1}^{m} |w_j|             $$\n",
    "L1 penalty for Lasso is defiined as the sum of the absolute magnitudes of the model weights shown above in the equations. Lasso selects at most $n$ features if $m>n$, where $n$ is the number of training samples and $m$ is the number of given features.Because of this feature of lasso, it avoids saturated models. Saturated model occurs if the number of training sammple is equal to the number of feature($m=n$) which causes overparameterization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35809b",
   "metadata": {},
   "source": [
    "# Decision Tree : Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb8df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573a9e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "cancer_data=load_breast_cancer()\n",
    "print(cancer_data.data) ## Featured values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3638808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(cancer_data.target) # target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d29d330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(cancer_data.data,cancer_data.target,random_state=42)\n",
    "dt=DecisionTreeClassifier(random_state=0,max_depth=4,criterion='entropy')\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39d4cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set 0.9929577464788732\n",
      "Accuracy on test set 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on training set {dt.score(X_train,y_train)}\")\n",
    "print(f\"Accuracy on test set {dt.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa301b2",
   "metadata": {},
   "source": [
    "We will try to visualize the tree created by the decision tree using export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad71b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14c2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will create a file called tree.dot which contains decision trees graphical plot\n",
    "export_graphviz(dt,out_file='tree.dot',class_names=['malignant','benign'],feature_names=cancer_data.feature_names,impurity=False,filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b79eb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"1061pt\" height=\"477pt\"\n",
       " viewBox=\"0.00 0.00 1061.00 477.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 473)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-473 1057,-473 1057,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#aed7f4\" stroke=\"black\" points=\"821.5,-469 635.5,-469 635.5,-401 821.5,-401 821.5,-469\"/>\n",
       "<text text-anchor=\"middle\" x=\"728.5\" y=\"-453.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">mean concave points &lt;= 0.051</text>\n",
       "<text text-anchor=\"middle\" x=\"728.5\" y=\"-438.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 426</text>\n",
       "<text text-anchor=\"middle\" x=\"728.5\" y=\"-423.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [158, 268]</text>\n",
       "<text text-anchor=\"middle\" x=\"728.5\" y=\"-408.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#46a3e7\" stroke=\"black\" points=\"627.5,-365 489.5,-365 489.5,-297 627.5,-297 627.5,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">worst radius &lt;= 16.83</text>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-334.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 267</text>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-319.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [16, 251]</text>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-304.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M673.31,-400.88C657.04,-391.12 639.11,-380.37 622.35,-370.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"624.08,-367.27 613.71,-365.12 620.48,-373.27 624.08,-367.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"619.77\" y=\"-385.68\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#e89051\" stroke=\"black\" points=\"910.5,-365 746.5,-365 746.5,-297 910.5,-297 910.5,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-349.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">worst perimeter &lt;= 114.45</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-334.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 159</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-319.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [142, 17]</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-304.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>0&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M760.97,-400.88C769.79,-391.89 779.44,-382.04 788.62,-372.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"791.35,-374.89 795.85,-365.3 786.35,-369.99 791.35,-374.89\"/>\n",
       "<text text-anchor=\"middle\" x=\"796.1\" y=\"-386.6\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#3d9fe6\" stroke=\"black\" points=\"398.5,-261 264.5,-261 264.5,-193 398.5,-193 398.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">radius error &lt;= 0.626</text>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 249</text>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [5, 244]</text>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M489.47,-298.98C463.68,-287.39 434.26,-274.17 407.85,-262.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"409.2,-259.08 398.64,-258.17 406.33,-265.46 409.2,-259.08\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#f6d1b7\" stroke=\"black\" points=\"630,-261 487,-261 487,-193 630,-193 630,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">worst texture &lt;= 19.91</text>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 18</text>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [11, 7]</text>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M558.5,-296.88C558.5,-288.78 558.5,-279.98 558.5,-271.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"562,-271.3 558.5,-261.3 555,-271.3 562,-271.3\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#3b9ee5\" stroke=\"black\" points=\"235,-157 86,-157 86,-89 235,-89 235,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">worst texture &lt;= 30.145</text>\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 246</text>\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 243]</text>\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M275.98,-192.88C259.62,-183.12 241.58,-172.37 224.73,-162.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.41,-159.24 216.03,-157.12 222.83,-165.25 226.41,-159.24\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#f2c09c\" stroke=\"black\" points=\"410,-157 253,-157 253,-89 410,-89 410,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">symmetry error &lt;= 0.025</text>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [2, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"331.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.5,-192.88C331.5,-184.78 331.5,-175.98 331.5,-167.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"335,-167.3 331.5,-157.3 328,-167.3 335,-167.3\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#399de5\" stroke=\"black\" points=\"105,-53 0,-53 0,0 105,0 105,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 213</text>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 213]</text>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M122.73,-88.95C111.96,-79.53 100.24,-69.27 89.52,-59.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.74,-57.19 81.91,-53.24 87.13,-62.46 91.74,-57.19\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#4da7e8\" stroke=\"black\" points=\"221.5,-53 123.5,-53 123.5,0 221.5,0 221.5,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 33</text>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [3, 30]</text>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.7,-88.95C165.75,-80.62 166.89,-71.65 167.97,-63.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.44,-63.6 169.23,-53.24 164.5,-62.72 171.44,-63.6\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"376,-53 263,-53 263,0 376,0 376,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"319.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"319.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [2, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"319.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.3,-88.95C326.25,-80.62 325.11,-71.65 324.03,-63.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"327.5,-62.72 322.77,-53.24 320.56,-63.6 327.5,-62.72\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#399de5\" stroke=\"black\" points=\"489,-53 394,-53 394,0 489,0 489,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"441.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"441.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"441.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M369.97,-88.95C380.94,-79.53 392.88,-69.27 403.8,-59.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"406.24,-62.41 411.54,-53.24 401.68,-57.1 406.24,-62.41\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#399de5\" stroke=\"black\" points=\"523,-149.5 428,-149.5 428,-96.5 523,-96.5 523,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"475.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"475.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 5]</text>\n",
       "<text text-anchor=\"middle\" x=\"475.5\" y=\"-104.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M531.55,-192.88C522.33,-181.56 512.02,-168.88 502.78,-157.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"505.29,-155.07 496.27,-149.52 499.86,-159.49 505.29,-155.07\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#ea985d\" stroke=\"black\" points=\"717.5,-157 541.5,-157 541.5,-89 717.5,-89 717.5,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"629.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">concave points error &lt;= 0.01</text>\n",
       "<text text-anchor=\"middle\" x=\"629.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 13</text>\n",
       "<text text-anchor=\"middle\" x=\"629.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [11, 2]</text>\n",
       "<text text-anchor=\"middle\" x=\"629.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M581.55,-192.88C587.63,-184.15 594.26,-174.62 600.6,-165.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"603.48,-167.51 606.32,-157.3 597.74,-163.51 603.48,-167.51\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"630,-53 517,-53 517,0 630,0 630,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"573.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 11</text>\n",
       "<text text-anchor=\"middle\" x=\"573.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [11, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"573.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M609.91,-88.95C604.76,-80.26 599.2,-70.86 594,-62.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"596.86,-60.06 588.75,-53.24 590.84,-63.62 596.86,-60.06\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#399de5\" stroke=\"black\" points=\"743,-53 648,-53 648,0 743,0 743,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"695.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"695.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 2]</text>\n",
       "<text text-anchor=\"middle\" x=\"695.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>11&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M652.58,-88.95C658.78,-80.07 665.49,-70.46 671.73,-61.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"674.67,-63.44 677.53,-53.24 668.93,-59.43 674.67,-63.44\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"#f7dac5\" stroke=\"black\" points=\"903,-261 754,-261 754,-193 903,-193 903,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-245.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">worst texture &lt;= 25.655</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-230.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 41</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-215.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [24, 17]</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-200.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M828.5,-296.88C828.5,-288.78 828.5,-279.98 828.5,-271.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"832,-271.3 828.5,-261.3 825,-271.3 832,-271.3\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"1034,-253.5 921,-253.5 921,-200.5 1034,-200.5 1034,-253.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"977.5\" y=\"-238.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 118</text>\n",
       "<text text-anchor=\"middle\" x=\"977.5\" y=\"-223.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [118, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"977.5\" y=\"-208.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>14&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M876.88,-296.88C894.55,-284.79 914.47,-271.15 931.89,-259.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"933.95,-262.06 940.22,-253.52 929.99,-256.28 933.95,-262.06\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"#50a9e8\" stroke=\"black\" points=\"921.5,-157 735.5,-157 735.5,-89 921.5,-89 921.5,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-141.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">worst concave points &lt;= 0.166</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-126.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 19</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-111.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [2, 17]</text>\n",
       "<text text-anchor=\"middle\" x=\"828.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M828.5,-192.88C828.5,-184.78 828.5,-175.98 828.5,-167.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"832,-167.3 828.5,-157.3 825,-167.3 832,-167.3\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"1053,-149.5 940,-149.5 940,-96.5 1053,-96.5 1053,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"996.5\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 22</text>\n",
       "<text text-anchor=\"middle\" x=\"996.5\" y=\"-119.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [22, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"996.5\" y=\"-104.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;19 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>15&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M883.04,-192.88C903.24,-180.62 926.04,-166.78 945.87,-154.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"947.74,-157.7 954.47,-149.52 944.1,-151.72 947.74,-157.7\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"#399de5\" stroke=\"black\" points=\"867.5,-53 769.5,-53 769.5,0 867.5,0 867.5,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"818.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 17</text>\n",
       "<text text-anchor=\"middle\" x=\"818.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [0, 17]</text>\n",
       "<text text-anchor=\"middle\" x=\"818.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = benign</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M825,-88.95C824.12,-80.62 823.17,-71.65 822.28,-63.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"825.76,-62.81 821.22,-53.24 818.8,-63.55 825.76,-62.81\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"999,-53 886,-53 886,0 999,0 999,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"942.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"942.5\" y=\"-22.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">value = [2, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"942.5\" y=\"-7.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">class = malignant</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>16&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M868.37,-88.95C879.74,-79.53 892.11,-69.27 903.43,-59.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"905.99,-62.31 911.45,-53.24 901.52,-56.92 905.99,-62.31\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x1d3a872c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## we will use graphviz library to open tree.dot file \n",
    "with open('tree.dot') as f:\n",
    "    dt_graph=f.read()\n",
    "display(graphviz.Source(dt_graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b6b7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay\n",
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a93633d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1d3ab5f89d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let us make the prediction on X_test \n",
    "y_pred=dt.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm_plot=ConfusionMatrixDisplay(cm)\n",
    "cm_plot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eee784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.96      0.93      0.94        54\n",
      "   malignant       0.96      0.98      0.97        89\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.96      0.95      0.96       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred,target_names=['benign','malignant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f241f05",
   "metadata": {},
   "source": [
    "# Exercise For Students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01592adb",
   "metadata": {},
   "source": [
    "In this exercise, we are going to import the wine datat from sklearn.datasets library. We have features for three different classes of wine. Create a model using decision tree and report the precision,recall and f1-score. Use max_depth=5,criterion= entropy and random_state=1000 for DecisionTreeClassifier object. Importing of the data has been done for you. \n",
    "\n",
    "Follow the follwing steps \n",
    "- Import the data\n",
    "- split the data using train_test_split\n",
    "- Create a model based on decision tre\n",
    "- Feed your training data on different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "966f3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9088f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data=load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "774c4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine =wine_data.data\n",
    "y_wine= wine_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa138fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a1141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.766px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
