{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6485ac",
   "metadata": {},
   "source": [
    "Installation Instruction:\n",
    "- Open Anaconda Navigator and open powershell terminal \n",
    "- git clone https://github.com/openai/gym   (make sure you have git )\n",
    "- cd gym\n",
    "- pip install -e .[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa19bd",
   "metadata": {},
   "source": [
    "# Reinforcement Learning(RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145eb30",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning(RL) is a generic framework for representing and solving control taks.It is concerned with solving sequnetial decision making problemssuch as video games,sports ,driving ,robotic control etc .Solving these kind of problems, we always have an objective/goal to fulfill. It takes many actions in sequences to reach our objective. Each action that has been taken would change the state of the environment/world.These changes could gives us some feedback(positive/negative) in return so as to decide on the next action to take.\n",
    "\n",
    "Imagine a situation where you are suppose to flag pole. You will be probably failing for first few times if you have no experience holding it . However we human being learn from our mistakes, and by making these mistakes you will be gathering lot of information related to balancing the flag pole such as center of gravity,rate at which flag is tilting over, how quickly you need to adjust, angle at which it falls. You take these information to make corrections in your every attempts.This is how Reinforcement Learning(RL) works. \n",
    "\n",
    "In RL , \"you\" are what is called an agent,the flag pole and your surrounding are called an \"environment\". Reinforcement learning is expressed as a system consisting of an agent and an environment. The environment produces information describing the state of the system which is known as state.An agent interacts with an environment  by observing the state and using this information to select an action. The environment accepts the action and moves into the next state and gives reward to the agent. One time step is equivalent time taken for this single cycle ($\\text{state} \\longrightarrow \\text {action}  \\longrightarrow \\text {reward} $) to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340db19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-21T09:25:47.874118Z",
     "start_time": "2022-03-21T09:25:47.829325Z"
    }
   },
   "source": [
    "![figure_1_1.png](images/figure_1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d13d9",
   "metadata": {},
   "source": [
    "Figure 1.1 shows the entire process as closed loop system of agent and environment working together.$s_t,r_t \\ \\text {and} \\ a_t$ are present state ,reward and action whereas $s_{t+1}\\ and \\ r_{t+1}$ are future state and reward.A policy is a functions which maps states to actions.RL has an objective to maximize the sum of rewards received by an agent by selecting good actions.The signals exchanged are $(s_t,a_t,r_t)$ tuple which is called an experience. The control loop can repeat forever or terminate by reaching  either a terminal state or a maximum time step $t=T$.An <b>episode</b> is defined as the time interval starting from  $t=0$ until the environment terminates.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e38264",
   "metadata": {},
   "source": [
    "<i>For simplicity we are going to drop the time step subscript and use standard notation followed by many books. Therefore a tuple denoting $(s_t,a_t,r_t)$ at current time $t$ will be written as  $(s,a,r)$  and a tuple denoting $(s_{t+1},a_{t+1},r_{t+1})$ at next time step $t+1$ will be written as $(s',a',r')$.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0166d5",
   "metadata": {},
   "source": [
    "# Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44d357",
   "metadata": {},
   "source": [
    "In RL framework, the agent makes its decisions as a function of a signal from the environment called environment's state. State in context of RL means whatever information is available to the agent. \n",
    "State signal could be the reading from the sensors or highly processed versions of these sensors readings.For example we can hear word \"no\" and movie into completely different states depending on the question that occured before \"no\" and the question is not audible anymore.Moreiver state signal is not exoected to inform the agent of everything about the environment.For example , If the agent is answering the phone , we should not expect it to know who the caller is in advance.We cannot blame agent for not knowing something that matters but only for having known something and then forgotten it.\n",
    "\n",
    "A state signal should be able to summarize the past sensations compactly in such a way that all relevant information.A state signal that succeeds in keeping all relevant information is said to be <b>Markov</b> and the signal is said to have <b>Markov property</b>.\n",
    "\n",
    "Markov property is  important in reinforcement learning because decisions and values are assumed to be a function only of the current state. For example : If we know the current state is cloudy, the most probable next state could be rainy. We do not need to know what sate occured before the cloudy. Moving from one state to another is called </b>transition</b> and the probability associated with particular state transition is called <b>transition probabilities</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449bb6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-21T07:17:26.230113Z",
     "start_time": "2022-03-21T07:17:26.225114Z"
    }
   },
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec1c00",
   "metadata": {},
   "source": [
    "An environment transits from one state to another using a transition function. A transition function is written as a Markov Decision Process(MDP) which models a sequential decision making. A reinforcemnt  learning follows Markov Property and thus it a part of Markov Decision Process(MDP). A transition to next state $s_{t+1}$ depends on the present state ${s_t}$ and action ${a_t}$. The transition function can be written as \n",
    "\n",
    "$$ p(s_{t+1}\\vert s_t,a_t) $$\n",
    "\n",
    "The Markov property states that the current state and action at time step $t$ has sufficient information to determine the transition probabilit for the next state $t+1$. MDP is defined by a  tuple of\n",
    "-  $\\mathcal{S}$   the set of states\n",
    "-  $\\mathcal{A}$  set of actions\n",
    "-  $\\mathcal{P(s_{t+1}\\vert s_t,a_t)}$  is the state transition function of the environment\n",
    "- $\\mathcal{R}(s_t,a_t,s_{t+1})$   set of reward function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b7141",
   "metadata": {},
   "source": [
    "The agent gets information about the transition function $\\mathcal{P(s_{t+1}\\vert s_t,a_t)}$  and reward function $\\mathcal{R}(s_t,a_t,s_{t+1})$ from the tuples $(s_t,a_t,r_t)$. Let us derive an <b>objective</b>  that an agent attempts to maximize. let us define a series $R(\\tau)$ for an <b>episode ,$\\tau = (s_0,a_0,r_0) \\ldots  (s_T,a_T,r_T)$\n",
    "   \n",
    " $$R(\\tau)=r_0+\\gamma r_1 +\\gamma^2 r_1 +\\gamma^3 r_3+\\ldots+\\gamma^T r_T = \\sum_{t=0}^T \\gamma^t r_t   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33371101",
   "metadata": {},
   "source": [
    "The equation 2 defines<b> return function</b> for trajectory $\\tau = (s_0,a_0,r_0) \\ldots  (s_T,a_T,r_T)$ where $\\gamma$ $\\in$  $[0,1] $ and $\\gamma$ is known as discount factor.  The objective $J(\\tau)$ is the averaged of returns over many episodes and defined as \n",
    " $$J(\\tau)= \\mathop{\\mathbb{E_{\\tau}\\bigg [ \\sum_{t=0}^T \\gamma^t r_t \\bigg  ]}} $$     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1d820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T15:00:36.045473Z",
     "start_time": "2022-04-01T15:00:36.037473Z"
    }
   },
   "source": [
    "When the discount factor $\\gamma$ is smaller , the less weights are given to the future rewards. Larger the $\\gamma$ more weights is given to rewards. Maximizing the return $R(\\tau)$ same as maximizing the objective $J(\\tau)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670de6c",
   "metadata": {},
   "source": [
    "Consider a mobile robot who has job of collecting empty soda cans inside of a restaurant. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them to recyling bins. This mobile runs on a rechargeable battery. The robot has control system, for interpreting sensory information, navigation, and controlling the arm and gripper.The decision for searching cans are made based on the current charge level of battery. The agent has to make a decision \n",
    "\n",
    "- Search for the can for a fixed period of time(<b>Search)\n",
    "- Stay stationary and wait for someone to bring the can by themself(<b>wait)\n",
    "- go back to docking station to recharge the battery(<b>recharge)\n",
    "\n",
    "The decision is made on period basis or whenever certain events occur.The agent has three actions(search , wait and recharge) and the state is determined by the state of the battery.The rewards might be zero most of the time and can become positive when th robot fetches an empty can or negative if the battery runs all the way down.\n",
    "The agent makes its decision as function of the enrgy level of the battery. It can distinguish two levels of battery(high and low), so the state set is $S=\\{ high,low \\}$.The agent's action sets are \n",
    "$$A(high)=\\{search,wait \\}$$\n",
    "$$A(low)=\\{search,wait,recharge\\}$$\n",
    "    \n",
    "The action set $A(high)$ doesnot contain recharge cause when the battery level is high enough we do not need to recharge it. When the energy level is high enough, then a robot can enter into the active search mode for the cans. When a robot enter into the state of Low,it takeas an action search mode with probability of $\\beta$ and it needs to rescued with probability of 1-$\\beta$ .\n",
    "Each can collected by the robot is counted as unit reward.If the robots needs to be rescued the reward would be -3(punished heavily). Rescuing means robot keeps on searching inspite of low battery, doesnot go to the charging station and the moment comes when it doesnot have enough juice to move and someone has to pick up  the robot and bring back to the docking station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54011cc",
   "metadata": {},
   "source": [
    "Figure1.2,Transition graph summarizes the dynamics of MDP process for recycling robot. There are two types of nodes present in the transition graph. One node shows the state nodes (High or Low) and other node shows the different actions that can be taken (search,wait and recharge)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4787de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T05:51:46.666968Z",
     "start_time": "2022-03-22T05:51:46.511747Z"
    }
   },
   "source": [
    "![figure_1_2.png](images/figure_1_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4009a1",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\n",
    "& \\text {Table1.1: Transistion probabilities and rewards for the MDP of recycling robot }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline \\text { s } & \\text { s' } & \\text {a} &  \\text{p(s'} \\mid \\text{s,a)} &  \\text{r(s,a,s')}\\\\\n",
    "\\hline high & high & search & \\alpha &\\text{r}_{search}  \\\\\n",
    "high & low & search & 1-\\alpha &\\text{r}_{search} \\\\\n",
    "low & high & search & 1-\\beta & -3 \\\\\n",
    "low & low & search & \\beta &\\text{r}_{search}\\\\\n",
    "high& high & wait & 1&\\text{r}_{wait} \\\\\n",
    "high& low & wait & 0&\\text{r}_{wait}  \\\\\n",
    "low& high & wait & 0&\\text{r}_{wait} \\\\\n",
    "low& low & wait & 1&\\text{r}_{wait} \\\\\n",
    "low& high & recharge & 1 &0 \\\\\n",
    "low& low & recharge &0 &0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a26b2c",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35769732",
   "metadata": {},
   "source": [
    "Deep learning algorithms such as Artificial Neural Network(ANN),Convolutional Neural Network(CNN) are able to process complex data efficiently.Deep learning algortihmsn can be applied to many problem types and tasks such as image classifications and predictions tasks.Some tasks to automate  such as driving a car would image processing but also the algorithm needs to learn how to act beside classifying nad predicting.The kind of problems where decisions must be made or some behavior needs to be set into motion are collectively called control tasks. Deep learning algorithms are popular algorithm used for RL becuase of their ability to process complex algorithm and thus it adopts  the name Deep Reinforcement Learning(DRL).RL is all about the type of problem and solution rather than about any particular learning algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b48bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T11:18:17.527838Z",
     "start_time": "2022-03-26T11:18:17.471492Z"
    }
   },
   "source": [
    "![figure_1_3.png](images/figure_1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852eeceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T11:06:08.300598Z",
     "start_time": "2022-11-01T11:06:08.283587Z"
    }
   },
   "source": [
    "## Cartpole-V0,v1: An environment from Open AI Gym "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14c9ef",
   "metadata": {},
   "source": [
    "Before we proceed with the various algorithm related to DRL, let us get familiar with the environment we are going to use for the excercise for all the problems. It is important to get a information about the environment we will be using. We will be using Cartpole-VO/V1 from openAI gym python package. A pole is stands on top of the movable cart.The goal of this cartpole environment is to keep the pole straight as long as possible and not let it drop on the ground.This could be achieved by moving the cart with certain velocity when the pole angle reaches to some position.\n",
    "Let us look at the observation space and action space for the given cartpole. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bbeaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T12:59:55.893923Z",
     "start_time": "2022-11-01T12:59:55.854924Z"
    }
   },
   "source": [
    "![cart_pole.gif](images/cart_pole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c96d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T13:12:56.843884Z",
     "start_time": "2022-11-01T13:12:56.822885Z"
    }
   },
   "source": [
    "<table>                                                                 \n",
    "  <thead>\n",
    "    <caption><b>OBSERVATION SPACE</b></caption>\n",
    "    <tr>\n",
    "      <th>Num</th>\n",
    "      <th>Observation</th>\n",
    "      <th>Min</th>\n",
    "      <th>Max</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>Cart Position</td>\n",
    "      <td>-4.8</td>\n",
    "      <td>4.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>Cart Velocity</td>\n",
    "      <td>-inf</td>\n",
    "      <td>inf</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>Pole Angle</td>\n",
    "      <td>~ -0.418 rad (-24°)</td>\n",
    "      <td>~ 0.418 rad (24°)</td>\n",
    "    </tr> \n",
    "     <tr>\n",
    "      <td>3</td>\n",
    "      <td>Pole Angular Velocity</td>\n",
    "      <td>-Inf</td>\n",
    "      <td>-Inf</td>\n",
    "    </tr> \n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173c623",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <caption><b>Action SPACE</b></caption>\n",
    "    <tr>\n",
    "      <th>Num</th>\n",
    "      <th>Action</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>Push cart to the left</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>Push cart to the right</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308f72c",
   "metadata": {},
   "source": [
    "These observation space is what creates our state space. Our state of a environment is based upon the cart position which can take values from [-4.8,4.8] ,cart velocity[-inf,inf],poleangle[-24,24] and pole Angular velocity[-inf,-inf].  What is the highest observation you can make from these variables? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d24405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T06:46:04.549693Z",
     "start_time": "2022-03-26T06:46:04.536691Z"
    }
   },
   "source": [
    "## Policy-Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fcfb9",
   "metadata": {},
   "source": [
    "A policy function $\\pi $  maps states to action probabilities which is used to sample an action such that  $a  \\sim  \\pi (s) $.  A policy $\\pi$ using a deep neural network as function approximators  consisting of learnable parameters  $\\theta$ is denoted as $\\pi_{\\theta}$.A neural network is capable to represent different policies parametrized by $\\theta$.A good policy learns good set of values for $\\theta$ which is improved through gradient ascent in parameter space. \n",
    "In eqaution 3, if the expectation is calculated over may trajectories sampled from a policy $\\pi$, then equation 3 can be written as \n",
    "$$J(\\pi_{\\theta})=  \\mathop{\\mathbb{E_{\\tau  \\sim  \\pi_\\theta }\\bigg [ \\sum_{t=0}^T \\gamma^t r_t \\bigg  ]}} \\\\\n",
    "= \\mathbb E_{\\tau  \\sim  \\pi_\\theta } [R(\\tau)]    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaf248",
   "metadata": {},
   "source": [
    "A policy based always tries to maximize the objective function $J(\\pi_{\\theta})$given in equation 6 by performing gradient ascent on the policies parameters $\\theta$. The gradient is computed to the direction of the steepest ascentand the caluclated gradient is use to update the parameter $\\theta$ as shown below \n",
    "$$\\theta \\leftarrow \\theta \\ + \\alpha  \\ \\nabla_{\\theta}J(\\pi_{\\theta})$$\n",
    "The policy gradient $\\nabla_{\\theta}J(\\pi_{\\theta})$ is defined as \n",
    "$$\\nabla_{\\theta}J(\\pi_{\\theta}) = \\mathbb E_{\\tau  \\sim  \\pi_\\theta } \\bigg [ \\sum_{t=0}^T R_t(\\tau)\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t\\vert s_t)\\bigg]  $$\n",
    "The R.H.S of equation 8 the gradient of objective function is the estimated value of gradient of log probability of the action with respect to $\\theta$ multiplied by return $R(\\tau)$. $\\pi_{\\theta}(a_t\\vert s_t)$ is the probability of action taken by agent at time step t . The action is sampled from the policy at given time $t$ i.e $a_t \\sim \\pi_{\\theta}(s_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157caba4",
   "metadata": {},
   "source": [
    "<b>REINFORCE</b> is one of the policy based algorithms. In REINFORCE  a trajectory is discarded after each parameter update and therefor REINFORCE is also called on-policy algorithm.Parameter update equation depends on the the current policy.REINFORCE algorithm is stated as \n",
    " <a id='demoboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    " - Initialize learning rate $\\alpha$\n",
    " - Initialize weighs $\\theta$ of policy's parameter learning Neural Network $\\pi_{\\theta}$\n",
    " - <b> for</b> episode =  $0 ,\\ldots ,\\text {MAX_EPISODE do} $\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample a trajectory $\\tau = s_0,a_0,r_0,\\ldots,s_T,a_T,r_T$\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set $\\nabla_{\\theta}J(\\pi_{\\theta})=0$\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>for</b> $t=0,\\ldots,T$ <b>do</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$R_t(\\tau)= \\sum_{t'=t}^T \\gamma^{t'-t}r'_t $\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\nabla_{\\theta}J(\\pi_{\\theta})=\\nabla_{\\theta}J(\\pi_{\\theta})+ R_t(\\tau)\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t\\vert s_t)$\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>end for</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta \\leftarrow \\theta \\ + \\alpha  \\ \\nabla_{\\theta}J(\\pi_{\\theta})$\n",
    " -<b>end for</b>    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869864e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:46.165282Z",
     "start_time": "2022-05-03T12:37:45.488404Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "env=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d4c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:46.180285Z",
     "start_time": "2022-05-03T12:37:46.167285Z"
    }
   },
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ee6fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:46.673257Z",
     "start_time": "2022-05-03T12:37:46.181283Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb70970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:46.688218Z",
     "start_time": "2022-05-03T12:37:46.678231Z"
    }
   },
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        self.layer1=nn.Linear(state_size,150)\n",
    "        self.layer2=nn.Linear(150,action_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.leaky_relu(self.layer1(x))\n",
    "        x=F.softmax(self.layer2(x))\n",
    "        return x\n",
    "net= model() # instantiate the class model  \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fc33e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:46.990218Z",
     "start_time": "2022-05-03T12:37:46.974224Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate=0.0009\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "#criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b911a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:47.512521Z",
     "start_time": "2022-05-03T12:37:47.503523Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "def discount_rewards(rewards):\n",
    "    lenr=len(rewards)\n",
    "    disc_return=torch.pow(gamma,torch.arange(lenr).float())*rewards\n",
    "    disc_return=disc_return/disc_return.max()\n",
    "    return disc_return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfeb49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:37:47.998727Z",
     "start_time": "2022-05-03T12:37:47.995031Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds,r):\n",
    "    return -1 * torch.sum(r*torch.log(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5328ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:39:04.240822Z",
     "start_time": "2022-05-03T12:37:48.658708Z"
    }
   },
   "outputs": [],
   "source": [
    "max_dur= 200\n",
    "max_episodes= 500 \n",
    "gamma = 0.99\n",
    "score=[]\n",
    "for episode in range(max_episodes):\n",
    "    curr_state= env.reset()\n",
    "    done=False\n",
    "    transitions=[]\n",
    "    print(f\"the episode is {episode}\")\n",
    "    for t in range (max_dur):\n",
    "        act_prob=net(torch.from_numpy(curr_state).float())\n",
    "        action=np.random.choice(np.array([0,1]),p=act_prob.data.numpy())\n",
    "        prev_state= curr_state\n",
    "        curr_state,_,done,info= env.step(action)\n",
    "        transitions.append((prev_state,action,t+1))\n",
    "        if done:\n",
    "            break\n",
    "        ep_len=len(transitions)\n",
    "        score.append(ep_len)\n",
    "        reward_batch=torch.Tensor([r for (s,a,r) in transitions]).flip(dims=(0,))\n",
    "        disc_rewards= discount_rewards(reward_batch)\n",
    "        state_batch=torch.Tensor([s for (s,a,r) in transitions])\n",
    "        action_batch=torch.Tensor([a for (s,a,r) in transitions])\n",
    "        pred_batch= net(state_batch)\n",
    "        prob_batch=pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "        loss=loss_fn(prob_batch,disc_rewards)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if t%5==0:\n",
    "            print(f\"time step {t} ----------- loss{loss}\")\n",
    "    print(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905f2dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T06:46:30.508811Z",
     "start_time": "2022-03-26T06:46:30.493635Z"
    }
   },
   "source": [
    "## Value-Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc087a",
   "metadata": {},
   "source": [
    "Value based algorithms evaluate state-action pairs $(s,a)$ by learning one of the value functions- $V^{\\pi}(s) \\ or \\ Q^{\\pi}(s,a) $. These value functions provide information about the objective and help an agent understand how good the states and actions are in terms of the expected future return.$V^{\\pi}(s) \\ or \\ Q^{\\pi}(s,a) $ is defined as \n",
    "\n",
    "$$V^{\\pi}(s)= \\mathbb{E_{s_0=s , \\tau  \\sim  \\pi } \\bigg [ \\sum_{t=0}^T \\gamma^t r_t \\bigg  ]}$$\n",
    "$$Q^{\\pi}(s,a)= \\mathbb{E_{s_0=s,a_0=a, \\tau  \\sim  \\pi } \\bigg [ \\sum_{t=0}^T \\gamma^t r_t \\bigg  ]}$$\n",
    "\n",
    "The value function $V^{\\pi}(s)$ evaluates how good or a bad a state is and measures the expected return from being in state $s$ assuming the agent is holding its current policy $\\pi$. The return $R(\\tau)= \\sum_{t=0}^T \\gamma^t r_t $ is measured from the current state $s$ to the end of the episode and all rewards received before current state $s$ are ignored(forward looking measure).\n",
    "\n",
    "The Q-value function $Q^{\\pi}(s,a)$ evaluates state- action pair. An agent acting with  a given current policy $\\pi$ , Q value function measures the expected return from taking action $a$ in state $s$.\n",
    "An agent learns either value functions $V^{\\pi}(s)$ or $Q^{\\pi}(s,a)$ from which an agent evaluates $(s,a)$ pairs and generate a policy.$Q^{\\pi}(s,a)$ is more commonly used value function than $V^{\\pi}(s)$ because of it's simplicity in coverting to a policy.$Q^{\\pi}(s,a)$ has the information about the paired states and actions $(s,a)$ whereas $V^{\\pi}(s)$  has only information about states. However the Q value functions  is also forward looking measure like $V^{\\pi}(s)$. An agent can calculate $Q^{\\pi}(s,a)$ for each of the actions $a \\in \\mathcal{A} $ in a particular state $s$ and select the action with maximum value. One of the disadvantage of $Q^{\\pi}(s,a)$ is the computational complexity for function approximation. It also needs lot of data to learn compared to $V^{\\pi}(s)$.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd21e5",
   "metadata": {},
   "source": [
    "### SARSA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676a394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T11:09:59.314125Z",
     "start_time": "2022-09-09T11:09:59.274125Z"
    }
   },
   "source": [
    "<b>SARSA (STATE-ACTION-REWARD-STATE-ACTION)</b> is one of the value based algorithm which learns $Q^{\\pi}(s,a)$ with a technique called Temporal Difference(TD). In temporal difference learning method, trajectories is generated and predict a $\\hat{Q}$-value for each $(s,a)$ pair.We use these trajectories to generate target Q-values $Q_{tar}$ and minimize the distance between $\\hat{Q}$ and $Q_{tar}$ using a standard loss function such as MSE. This is similar to supervised learning of machine learning however the difference between them is that the target value are already available in supervise ML dataset whereas we need to generate the target value in SARSA.\n",
    "\n",
    "If we have N trajectories $\\tau$ $i\\in {1,.......,N}$ starting in state $s$ with agent taking action $a$,then MonteCarlo(MC) estimate of target Q is the average of all trajectories returns and is given as \n",
    "$$Q_{tar}^\\pi (s,a)=\\frac{1}{N}\\sum_{i=1}^N R(\\tau_i)$$\n",
    "\n",
    "After calculating the$Q_{tar}$, dataset can be called as a labeled data because each(s,a) pair in the dataset has the target value associated with it. However the disadvantage of using MC is that the agent has to wait for episodes to end before it can calculate the target values. An episode can have many time steps which increases the training time.\n",
    "\n",
    "To overcome this disadvantage, we introduce the Temporal Difference learning Q values.In TD, Q values for the current timestep can be written in terms of Q values for the next time step. Mathematically, it can be stated as \n",
    "$$Q^{\\pi}(s,a)=r+\\gamma Q^{\\pi}(s',a')=Q_{tar}^\\pi (s,a) $$\n",
    "\n",
    "The equation(12) is known as Bellman equation. In equation (12), only the information from the next step in the environment is required to calculate the target values instead of whole trajectory of a single episode(MC method).This learning is a bootstrapped learning as the current estimate of Q values are used to calculate the target values.We have an example present in section 3.3.2 which shows how this bellman equation/Temporal Difference works in the SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579f6f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T09:10:39.893328Z",
     "start_time": "2022-11-01T09:10:39.871329Z"
    }
   },
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5b1ef",
   "metadata": {},
   "source": [
    "#### Exploration and Exploitation -  $\\epsilon$-greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f51512",
   "metadata": {},
   "source": [
    "We need to compute the $Q$ values for all the possible actions in that state inorder to identify the maximum Q values.Suppose we have learned the optimal Q function values then the agent will always select particular action which corresponds to the maximum Q value in each state. We called this as a <b>greedy policy</b>.\n",
    "\n",
    "However acting always greedily means that the agent might not be able to explore the entire state-action space sufficiently.If an agent always select the same action $a$ in state $s$, then there may be another optimal action in state $s$ which agent could never get the chance to experience/explore. Due to which Q values estimate for some $(s,a)$ pairs may be inaccurate and agent can make sub optimal actions and stuck in a local minimum.\n",
    "\n",
    "$\\epsilon$-greedy policy algorithm gives an agent a chance to  explore along with exploit. In this policy, an agent selects the greedy action(exploitation) with probability $1-\\epsilon$ and acts randomly with probability $\\epsilon$ which is also known as exploration probability.That means in every 100 runs, the chances of choosing a random action at the given state is $\\epsilon \\times 100$.This also  means even if there is an optimal Q value for the given state-action , an agent might randomly choose different action. This dilemma of chosing a  best actions or  a random action is known as exploration-exploitation tradeoff.\n",
    "Exploration causes poor performance for some time but however increases the chances of finding optimal $Q$ values for state-action space. \n",
    "\n",
    "SARSA is an ON policy algorithm which uses $\\epsilon-greedy$ policy. In the equation(12),  $Q^{\\pi}(s',a')$ values can differ based on which policy it chooses for $a'$. An agent cannot generate experience using different policy in case of SARSA. For example if we have two policies $\\pi^1$(from current policy) and  $\\pi^2$(from older policy) which are used to gather experiences $(s,a,r,s',a_1')$ ,$(s,a,r,s',a_2')$ respectively , which has Q value $Q^{\\pi^1}(s,a)$ and $Q^{\\pi^2}(s,a)$ then the update of $Q^{\\pi^1}(s,a)$ will be incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T08:54:18.349188Z",
     "start_time": "2022-11-01T08:54:18.335170Z"
    }
   },
   "source": [
    "### Q Learning process : An example with simple environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9cf01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T08:51:57.053739Z",
     "start_time": "2022-11-01T08:51:57.010546Z"
    }
   },
   "source": [
    "![figure_1_4.png](images/figure_1_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08e48d",
   "metadata": {},
   "source": [
    "To understand the Q learning process or how the Q tables are updated we are going to take a simple environment as shown in Figure 1.4. This environment has five states $S_{T1},S_{1},S_{2},S_{3},S_{T2}$ and two actions $\\text{(U)Up,(D)Down} $. The rule for this environment is very simple. A person in a given state has to reach a terminal $S_{T2}$ where he is rewarded.For the given environment, the person can do so by simply moving down no matter what states he is in . However there could be more complex environment with large action space. Another terminal state is $S_{T1}$ where the person should not be and will be no reward for reaching that state. If a person reaches $S_{T1},S_{T2}$terminal state  , it is the end of the episode and a person restarts the environment all over again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bf42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T08:52:40.547077Z",
     "start_time": "2022-11-01T08:52:40.500081Z"
    }
   },
   "source": [
    "![figure_1_5.png](images/figure_1_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39103f27",
   "metadata": {},
   "source": [
    "In Figure1.5, we can see the process of how the Q tables has been updated, in each episode. However whenever a new episodes starts, the Q table uses previous episode value of $Q(s,a)$  has been used to get the new updated $Q^{*}(s,a)$. \n",
    "Now let us look at the SARSA algorithm when a Neural Network(NN) is used as an agent down below ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08289d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T09:46:11.593255Z",
     "start_time": "2022-11-01T09:46:11.573298Z"
    }
   },
   "source": [
    "<a id='demoboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    " - Initialize learning rate $\\alpha$\n",
    " - Initialize $\\epsilon$\n",
    " - Randomly initialize the network parameters $\\theta$\n",
    " - <b> for</b> m =  $1 ,\\ldots ,\\text {MAX_EPISODE do} $\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gather N experiences $(s_i,a_i,r_i,s'_i,a'_i)$ using the current $\\epsilon$-greedy         policy \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>for</b> $i=1,\\ldots,N$ <b>do</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Calculate target Q-values for each example\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y_i=r_i+\\delta_{s'_{i}}\\gamma Q^{\\pi_{\\theta}}(s'_{i},a'_{i})$ where $\\delta_{s'_{i}}$ =0 if $s'_{i}$ is terminal ,1 otherwise\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>end for</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # calculate the MSE    \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$L(\\theta)=\\frac{1}{N}\\sum_{i}(y_i-Q^{\\pi_{\\theta}}(s_{i},a_{i}))^2$\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Update the networks paramereter  \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta=\\theta-\\alpha\\nabla_{\\theta}L(\\theta)$ \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Decay $\\epsilon$   \n",
    " -<b>end for</b>    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4056c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T11:10:26.339768Z",
     "start_time": "2022-09-09T11:10:26.322767Z"
    }
   },
   "source": [
    "### Deep - Q Network(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce78d4",
   "metadata": {},
   "source": [
    "Deep-Q Network is a value based temporal difference(TD) algorithm which approximates the Q-function.The learned Q function is used by an agent to select actions. DQN is applied when the action space is discrete.DQN learns the optimal Q function instead of Q function for the current policy(SARSA).DQN are off policy which learns from the experience gathered by an agent rather than data gathering policy.A small change to the SARSA algorithm results in the DQN algorithm but with improved stability and speed of learning.\n",
    "Similar to SARSA, Deep-Q learns the Q function using TD learning. The bellman equation for DQN can be written as \n",
    "$$Q^{\\pi}(s,a)=r+\\gamma \\underset{a'_i}{max}Q^{\\pi}(s',a')=Q_{tar}^\\pi (s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63192c76",
   "metadata": {},
   "source": [
    "SARSA is ON-policy because it always uses the current policy to get the action $a'$ for next state $s'$ to calculate Q values for the next state. In equation (13),we have a term $\\underset{a'_i}{max}Q^{\\pi}(s',a')$ , which actually means to take only one maximum Q value for all possible actions in state $s'$. So that means the value of$Q^{\\pi}(s',a')$ is independent of what policy we choose to get action $a'$ and therefore DQN is off policy algorithm.\n",
    "\n",
    "Eventhough DQN is off policy algorithm,it still uses a policy to gather experience. The policy used by a DQN agent should visit states that are reasonably similar to those that would be visited by acting greedly with agents current estimate of optimal policy. $\\epsilon$ greedy policy can be used with DQN. \n",
    "\n",
    "The algorithm for DQN is stated below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267d321",
   "metadata": {},
   "source": [
    "\n",
    " <a id='demoboundary'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    " - Initialize learning rate $\\alpha$\n",
    " - Initialize $\\epsilon$\n",
    " - Initialize number of batches per training step,B\n",
    " - Initialize batch size N\n",
    " - Initialize number of updates per batch,U\n",
    " - Initialize $\\tau$\n",
    " - Initialize experience replay memory with max size K   \n",
    " - Randomly initialize the network parameters $\\theta$\n",
    " - <b> for</b> m =  $1 ,\\ldots ,\\text {MAX_EPISODE do} $\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gather and store $h$ experiences $(s_i,a_i,r_i,s'_i)$ using the current $\\epsilon$-greedy         policy \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>for</b> $b=1,\\ldots,B$ <b>do</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>for</b> $u=1,\\ldots,U$ <b>do</b>    \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>for</b> $i=1,\\ldots,N$ <b>do</b>  \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Calculate target Q-values for each example\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y_i=r_i+\\delta_{s'_{i}}\\gamma\\underset{a'_i}{max} Q^{\\pi_{\\theta}}(s'_{i},a'_{i})$ where $\\delta_{s'_{i}}$ =0 if $s'_{i}$ is terminal ,1 otherwise\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>end for</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # calculate the MSE    \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$L(\\theta)=\\frac{1}{N}\\sum_{i}(y_i-Q^{\\pi_{\\theta}}(s_{i},a_{i}))^2$\n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Update the networks paramereter  \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta=\\theta-\\alpha\\nabla_{\\theta}L(\\theta)$ \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>end for</b>   \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>end for</b> \n",
    " - &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Decay $\\epsilon$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1640663",
   "metadata": {},
   "source": [
    "# Deep -Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340942c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:50.477976Z",
     "start_time": "2022-07-12T15:32:47.578440Z"
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import gym \n",
    "import numpy as np \n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2766296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:50.493951Z",
     "start_time": "2022-07-12T15:32:50.478952Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumud\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "episodes=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359e7514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:50.509950Z",
     "start_time": "2022-07-12T15:32:50.495947Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.95 \n",
    "        self.epsilon=1 # exploration rate\n",
    "        self.epsilon_min=0.01\n",
    "        self.epsilon_decay=0.995\n",
    "        self.learning_rate=0.001\n",
    "        self.model=self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def act(self,state):\n",
    "        if np.random.rand()>=self.epsilon:  # exploration \n",
    "            print('Exploring')\n",
    "            return random.randrange(self.action_size)\n",
    "        #act_values=self.model.predict(state)\n",
    "        print('Exploitation')\n",
    "        return np.argmax(self.model.predict(state)[0]) # exploitation\n",
    "        \n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def replay(self,batch_size):\n",
    "        minibatch= random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            target=reward\n",
    "            if not done:\n",
    "                target = (reward+self.gamma*np.amax(self.model.predict(next_state)[0]))\n",
    "                target_f=self.model.predict(state)\n",
    "                target_f[0][action]=target\n",
    "                #print(f\"state{state}-------target{target}\")\n",
    "                self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "            if self.epsilon>self.epsilon_min: \n",
    "                self.epsilon*=self.epsilon_decay  # linear decay\n",
    "                    \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484d30b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:50.525953Z",
     "start_time": "2022-07-12T15:32:50.511948Z"
    }
   },
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "done=False\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f557e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:51.426311Z",
     "start_time": "2022-07-12T15:32:50.526951Z"
    }
   },
   "outputs": [],
   "source": [
    "agent=DQN_Agent(state_size,action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca56537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:51.442305Z",
     "start_time": "2022-07-12T15:32:51.430304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ffa4ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T17:18:58.818861Z",
     "start_time": "2022-07-12T15:32:51.443308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 True\n",
      "episodes 0/100, score: 9 ,e:1\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 True\n",
      "episodes 1/100, score: 8 ,e:1\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 True\n",
      "episodes 2/100, score: 9 ,e:1\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 True\n",
      "episodes 3/100, score: 8 ,e:0.4484282034609769\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 4/100, score: 29 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 5/100, score: 10 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 6/100, score: 24 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 7/100, score: 30 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 8/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 9/100, score: 26 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 10/100, score: 16 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 11/100, score: 21 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 12/100, score: 21 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 13/100, score: 24 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 14/100, score: 17 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 15/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 16/100, score: 9 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 17/100, score: 11 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 18/100, score: 37 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 19/100, score: 27 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 20/100, score: 31 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 21/100, score: 69 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 22/100, score: 11 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 23/100, score: 36 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 24/100, score: 16 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 25/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 26/100, score: 19 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 27/100, score: 25 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 28/100, score: 33 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 29/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 30/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 31/100, score: 16 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 32/100, score: 27 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 33/100, score: 23 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 34/100, score: 36 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 35/100, score: 15 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 36/100, score: 11 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 37/100, score: 16 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 38/100, score: 43 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 39/100, score: 13 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 40/100, score: 34 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 41/100, score: 10 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 42/100, score: 73 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 43/100, score: 34 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 44/100, score: 15 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 45/100, score: 26 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 46/100, score: 16 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 47/100, score: 10 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 48/100, score: 20 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 49/100, score: 14 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 50/100, score: 22 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 51/100, score: 23 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 52/100, score: 17 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 53/100, score: 24 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 54/100, score: 8 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 55/100, score: 10 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 56/100, score: 10 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 57/100, score: 46 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 58/100, score: 10 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 59/100, score: 15 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 60/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 61/100, score: 14 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 62/100, score: 19 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 63/100, score: 25 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 64/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 65/100, score: 13 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 66/100, score: 51 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 67/100, score: 13 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 68/100, score: 14 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 69/100, score: 46 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 70/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 71/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 72/100, score: 17 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 73/100, score: 20 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 74/100, score: 8 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 75/100, score: 22 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 76/100, score: 19 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 77/100, score: 15 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 78/100, score: 14 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 79/100, score: 25 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 80/100, score: 13 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 81/100, score: 13 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 82/100, score: 21 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 83/100, score: 46 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 84/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 85/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 86/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 87/100, score: 12 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 88/100, score: 15 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 89/100, score: 15 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 90/100, score: 18 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 91/100, score: 26 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 92/100, score: 9 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 93/100, score: 17 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 94/100, score: 19 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 95/100, score: 71 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 96/100, score: 31 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 97/100, score: 14 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploitation\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 98/100, score: 21 ,e:0.00998645168764533\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 False\n",
      "Exploring\n",
      "1.0 True\n",
      "episodes 99/100, score: 18 ,e:0.00998645168764533\n"
     ]
    }
   ],
   "source": [
    "rwd=[]\n",
    "for e in range(episodes):\n",
    "    \n",
    "    state=env.reset()\n",
    "    state=np.reshape(state,[1,state_size])\n",
    "    rwd_cum=0\n",
    "    for time in range(500):\n",
    "        \n",
    "        action=agent.act(state) # choose between exploration and exploitation\n",
    "        #env.render() -- make sure you have enough resources to render. or you can just comment it out \n",
    "        next_state,reward,done,_=env.step(action)\n",
    "        print(reward,done)\n",
    "        reward = reward if not done else -10\n",
    "        rwd_cum=reward+rwd_cum\n",
    "        next_state=np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(f\"episodes {e}/{episodes}, score: {time} ,e:{agent.epsilon}\")\n",
    "            break\n",
    "        #print(len(agent.memory))\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    rwd.append(rwd_cum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b532e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T14:15:30.906100Z",
     "start_time": "2022-10-07T14:15:30.902101Z"
    }
   },
   "source": [
    "# Exercise For Students "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ca377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T14:16:32.073360Z",
     "start_time": "2022-10-07T14:16:32.069365Z"
    }
   },
   "source": [
    "In this exercise, students are suppose to modified above deep Q learning algorithm to SARSA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ac532",
   "metadata": {},
   "source": [
    "# Solution : SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f882f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T11:12:09.958043Z",
     "start_time": "2022-07-12T11:12:09.946043Z"
    }
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import gym \n",
    "import numpy as np \n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8e45a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T11:12:10.349170Z",
     "start_time": "2022-07-12T11:12:10.340181Z"
    }
   },
   "outputs": [],
   "source": [
    "class SARSA_Agent():\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.95 \n",
    "        self.epsilon=1.0 # exploration rate\n",
    "        self.epsilon_min=0.01\n",
    "        self.epsilon_decay=0.995\n",
    "        self.learning_rate=0.001\n",
    "        self.model=self._build_model()\n",
    "        self.epsiolon_greedy=True\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def act(self,state,decay_step):\n",
    "        if self.epsilon_greedy:\n",
    "            explore_probability=self.epsilon_min+(self.epsilon-self.epsilon_min)*np.exp(-self.epsilon_decay*decay_step)\n",
    "        if np.random.rand()<=self.epsilon:  # exploration \n",
    "            return random.randrange(self.action_size)\n",
    "        #act_values=self.model.predict(state)\n",
    "        return np.argmax(self.model.predict(state)[0]) # exploitation\n",
    "        \n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,next_action,done):\n",
    "        self.memory.append((state,action,reward,next_state,next_action,done))\n",
    "    \n",
    "    def replay(self,batch_size):\n",
    "        minibatch= random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,next_action,done in minibatch:\n",
    "            target=reward\n",
    "            if not done:\n",
    "                target = (reward+self.gamma*np.amax(self.model.predict(next_state)[0][next_action]))\n",
    "                target_f=self.model.predict(state)\n",
    "                target_f[0][action]=target\n",
    "                #print(f\"state{state}-------target{target}\")\n",
    "                self.model.fit(state,target_f,epochs=1,verbose=1)\n",
    "            if self.epsilon>self.epsilon_min: \n",
    "                self.epsilon*=self.epsilon_decay  # linear decay\n",
    "                    \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e3799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T11:12:10.789105Z",
     "start_time": "2022-07-12T11:12:10.765110Z"
    }
   },
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "episodes=100\n",
    "done=False\n",
    "batch_size=32\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec66a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T11:12:12.113129Z",
     "start_time": "2022-07-12T11:12:11.154009Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_SARSA=SARSA_Agent(state_size,action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f02ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T11:16:10.883901Z",
     "start_time": "2022-07-12T11:12:12.114108Z"
    }
   },
   "outputs": [],
   "source": [
    "rwd=[]\n",
    "for e in range(episodes):\n",
    "    decay_step=0\n",
    "    state=env.reset()\n",
    "    state=np.reshape(state,[1,state_size])\n",
    "    rwd_cum=0\n",
    "    #action=agent_SARSA.act(state)\n",
    "    for time in range(500):\n",
    "        decay_step+=1\n",
    "        action=agent_SARSA.act(state,decay_step) # choose between exploration and exploitation\n",
    "        #env.render() -- make re you have enough resources to render. or you can just comment it out \n",
    "        next_state,reward,done,_=env.step(action)\n",
    "        #print(reward,done)\n",
    "        reward = reward if not done else -10\n",
    "        rwd_cum=rwd_cum+reward\n",
    "        print(reward,next_state)\n",
    "        next_state=np.reshape(next_state,[1,state_size])\n",
    "        next_action=agent_SARSA.act(next_state)\n",
    "        agent_SARSA.remember(state,action,reward,next_state,next_action,done)\n",
    "        state=next_state\n",
    "        if done:\n",
    "            print(f\"episodes {e}/{episodes}, score: {time} ,e:{agent_SARSA.epsilon}\")\n",
    "            break\n",
    "        #print(len(agent.memory))\n",
    "        if len(agent_SARSA.memory) > batch_size:\n",
    "            agent_SARSA.replay(batch_size)\n",
    "    rwd.append(rwd_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc2534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T15:32:25.214451Z",
     "start_time": "2022-07-12T15:32:25.204453Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc648596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.952px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
