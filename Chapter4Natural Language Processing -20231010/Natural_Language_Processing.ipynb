{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5d58ef",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f5e1d",
   "metadata": {},
   "source": [
    "Natural Language Processing(NLP) is about the application of computers on different language nuances and to build real-world applications using NLP techniques. We can think of NLP as an analogous to teaching a new language to a child.The most common tasks a child learn from his teachers are understanding words, sentences and forming grammatically and structurally correct sentences. NLP can be classified as a subset of the broader field of speech and language processing. \n",
    "NLP is the field dedicated exclusively to the automated understanding of human language. The few classification problem that are very much common to NLP are :\n",
    "\n",
    "* Given characters of a corpus  to predict where words start and end.\n",
    "* Given words of a corpus to predict where sentences start and end.\n",
    "* Given words in a sentence to predict part-of-speech for each word. \n",
    "* Given words in a sentence to predict where phrases start and ends.\n",
    "* Given words in a sentence to predict where named entity (nouns) references start and end.\n",
    "* Given words find the sentiment in the sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3027b",
   "metadata": {},
   "source": [
    "In brief, NLP is suppose to do one of these three tasks\n",
    "* Label a region of text (Part of Speech Tagging or sentiment classification).\n",
    "* Link two or more region of text which are referencing to the same real world thing.\n",
    "* try to fill in missing information (words) based on context.\n",
    "With the popularity of Deep Learning, NLP now uses state-of-art algorithms such as RNN,LSTM,Transformers, etc. Before that, NLP used advanced ,probabilistic and non parametric models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c6ace5",
   "metadata": {},
   "source": [
    "NLP is a subfield of Artificial Intelligence(AI) which deals with the natural language and it is at the intersection of AI and linguistics.It enables the machine to understand the language of humans and thus bridge the communication gap between human and the machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9a1db",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit(NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97467417",
   "metadata": {},
   "source": [
    "NLTK was created in 2001 for a Linguistic Course at the University of Pennysylvania. It has been widely adopted toolkit for research projects in the field of Natural Language Processing. NLTK comes with many modules for different language processing tasks such as String Preprocessing, POS(part-of-Speech tagging),classification,parsking,chunking and so on . Eventhough the toolkit is efficient enought to do meaningful tasks , it has not been optimized for runtime performance.\n",
    "NLTK is a free toolkit and is donwnloadable from http://www.nltk.org/. You can find the detail instruction on downloading and installing the toolkit from BIG FLash moodle page inside <u>Introduction to the ML/AI MOOC tile</u>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee822a17",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31e032",
   "metadata": {},
   "source": [
    "Spacy is the library for advanced NLP in python and cython. Spacy packages includes state-of-the-art speed and neural netwrok models for tagging ,parsing,named entity recognition,text classification.It also includes pretrained transformers like BERT. IT is open source software and is downloadlable from https://spacy.io/usage. You can find the detail instruction on downloading and installing the toolkit from BIG FLash moodle page inside <u>Introduction to the ML/AI MOOC tile</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abe897",
   "metadata": {},
   "source": [
    "# Processing Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1b8bb",
   "metadata": {},
   "source": [
    "When grading the answers in the exam questions, the grader grades the answer based on the relevant part of the answers which carries most of information related to question and ignores irrelevant part.The grader identifies the key words in the questions and try to match them to the answer to find the correct one . Text processing works in the similar strategy.The machine doesnot need irrelevant part of the corpora(collection of texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339d73e",
   "metadata": {},
   "source": [
    "##  Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe6c2a",
   "metadata": {},
   "source": [
    "In NLP, we need to divide the texts into multiple sentences or words. Usually computer reads bodies of text due to which we need to separate this single body of texts as an individual string object. Tokenization is the process of splitting each documents into the words that appears in the docoment. For example splitting the documents on whitespaces and punctuation.\n",
    "\n",
    "In the example below, we are going to look at the two different tokenization method from nltk library word_tokenize and sent_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4599de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''I am a student from the University of Alabama. I\n",
    "was born in Ontario, Canada and I am a huge fan of the United States. I am going to get a degree in Philosophy to improve\n",
    "my chances of becoming a Philosophy professor. I have been\n",
    "working towards this goal for 4 years. I am currently enrolled\n",
    "in a PhD program. It is very difficult, but I am confident that\n",
    "it will be a good decision'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3577b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sample_word_tokens = word_tokenize(sample_text)\n",
    "sample_sent_tokens = sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c37045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'student', 'from', 'the', 'University', 'of', 'Alabama', '.', 'I', 'was', 'born', 'in', 'Ontario', ',', 'Canada', 'and', 'I', 'am', 'a', 'huge', 'fan', 'of', 'the', 'United', 'States', '.', 'I', 'am', 'going', 'to', 'get', 'a', 'degree', 'in', 'Philosophy', 'to', 'improve', 'my', 'chances', 'of', 'becoming', 'a', 'Philosophy', 'professor', '.', 'I', 'have', 'been', 'working', 'towards', 'this', 'goal', 'for', '4', 'years', '.', 'I', 'am', 'currently', 'enrolled', 'in', 'a', 'PhD', 'program', '.', 'It', 'is', 'very', 'difficult', ',', 'but', 'I', 'am', 'confident', 'that', 'it', 'will', 'be', 'a', 'good', 'decision']\n"
     ]
    }
   ],
   "source": [
    "print(sample_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b35408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a student from the University of Alabama.', 'I\\nwas born in Ontario, Canada and I am a huge fan of the United States.', 'I am going to get a degree in Philosophy to improve\\nmy chances of becoming a Philosophy professor.', 'I have been\\nworking towards this goal for 4 years.', 'I am currently enrolled\\nin a PhD program.', 'It is very difficult, but I am confident that\\nit will be a good decision']\n"
     ]
    }
   ],
   "source": [
    "print(sample_sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9550d0",
   "metadata": {},
   "source": [
    "The difference between word_tokenize and sent_tokenize is sent_tokenize tokenizes the text by sentence delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9122e7",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30cad0",
   "metadata": {},
   "source": [
    "Similary some words in the corpus contains singular or plural version. For example , the semantics of 'drawer' and 'drawers' are so close that distinguishing them as a seperate word will create an overfitting.This problem can be overcome by using the word stem by identifying all the words that have the same word stem.The process of removing the suffixes at the end of the words is called stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4a3f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T18:54:25.357403Z",
     "start_time": "2022-11-01T18:54:12.032775Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da997ecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-01T18:58:25.963586Z",
     "start_time": "2022-11-01T18:58:25.934580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  given_word stem_word\n",
      "0    annoyed     annoy\n",
      "1  levitated     levit\n",
      "2        was        wa\n",
      "3       cats       cat\n",
      "4     better    better\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "words=['annoyed','levitated','was','cats','better']\n",
    "stems=[ stemmer.stem(word=word) for word in words]\n",
    "df_stem=pd.DataFrame({'given_word':words,\n",
    "               'stem_word':stems\n",
    "             })\n",
    "print(df_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f71d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-03T12:28:37.917588Z",
     "start_time": "2022-06-03T12:28:37.912588Z"
    }
   },
   "source": [
    "In the result above, you can see that stemmer only drops the sufix at the end of word. However the stem of the word 'was' is 'be' not 'wa'. This is the limitation is stemming process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec265287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T12:37:38.979518Z",
     "start_time": "2022-01-24T12:37:38.976520Z"
    }
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea82f2",
   "metadata": {},
   "source": [
    "Lemmatization is closely related to the stemming process. Stemming tends to get the stem words without taking the context of the word in the sentence by simply dropping the suffix. However Lemmatization applies morphohlogical analysis to the words and takes the role of the word into account. Both Stemming and Lemmatization are the form of normalization which try to extract normal form of words.\n",
    "\n",
    "In the example below, we are going to use the same list of words that we had used in stemming example and see the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7232d8ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T17:30:29.809471Z",
     "start_time": "2022-11-02T17:30:16.695366Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09b1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrd_lmtz=WordNetLemmatizer()\n",
    "words=['annoyed','levitated','was','cats','better']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549aa680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T17:30:30.414783Z",
     "start_time": "2022-11-02T17:30:29.810442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  given_word lematize_word\n",
      "0    annoyed         annoy\n",
      "1  levitated      levitate\n",
      "2        was            be\n",
      "3       cats           cat\n",
      "4     better        better\n"
     ]
    }
   ],
   "source": [
    "lmtz= [wrd_lmtz.lemmatize(word,pos='v')  for word in words ] # By default WordNetLemmatizer lemattizes only n=noun words\n",
    "df_lmtz=pd.DataFrame({'given_word':words,\n",
    "               'lematize_word':lmtz\n",
    "             })\n",
    "print(df_lmtz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c2f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-24T12:42:16.427772Z",
     "start_time": "2022-01-24T12:42:16.416716Z"
    }
   },
   "source": [
    "##  Stop Words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c961ebe",
   "metadata": {},
   "source": [
    "While processing the raw text data it is important to get rid of uninformative words by discarding words which are too frequent to be informative. we can import the stopword from NLTK package. All of the stopwords available in the NLTK are lowercase by default. <b>Therefore it is importantto convert all the words in the tokens in the lowercase letter because the matching process of words in the text and in the list of stopwords are case sensitive.</b>\n",
    "\n",
    "Below is an example of removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f3419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255bf58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop=stopwords.words('english') \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b7f95",
   "metadata": {},
   "source": [
    "We can see above the list of stopwords for the english corpus. We will be removing the stop words from the sample_sent_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d05adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student-university-alabama.', 'born-ontario,-canada-huge-fan-united-states.', 'going-get-degree-philosophy-improve-chances-becoming-philosophy-professor.', 'working-towards-goal-4-years.', 'currently-enrolled-phd-program.', 'difficult,-confident-good-decision']\n"
     ]
    }
   ],
   "source": [
    "sent_stop_rmv=[]\n",
    "for sent in sample_sent_tokens:\n",
    "    clean_word=[]\n",
    "    for word in sent.split():\n",
    "        if word.lower() not in stop:\n",
    "            clean_word.append(word.lower()) # if true append the lower case words\n",
    "    sent_stop_rmv.append('-'.join(clean_word)) # we can use white spaces aswell while joining\n",
    "print(sent_stop_rmv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336bed5",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45666a23",
   "metadata": {},
   "source": [
    "Natural Language Processing is about preparing textual data for machine learning and deep learning models.However ML/DL model works efficiently with the numerical data as input and therefore it becomes important to transform the preprocess textual data into numerical data. The embedded numerical data are the representation of the textual data and consists of real-value vectors. The words which has similar meaning are mapped to similar vectors. Word embeddings are generated once the tokenization has been performed on the corpus. \n",
    "One of the important reason for embeddings is to make machine understand the synonyms the way human do. For example : a machine has to learn to differentiate between positive and negative adjective or to familariza with different words that has similar meaning and either gives positive or negative impact. Let us take two sentences:\n",
    "* The food here is good.\n",
    "\n",
    "* The food here is great.\n",
    "\n",
    "Both of above sentence indicates positive vibes about the food and thus word embedding maps 'good' and 'great' two seperate but similar real-value vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27334683",
   "metadata": {},
   "source": [
    "###  CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bf7c4",
   "metadata": {},
   "source": [
    "CountVectorizer transforms a text into a vector on the basis of the frequency(count) of each words present in the text or in the corpus. A count vectorizer is an implementation of bag-of-words in which we code text data as a representation of features/words.\n",
    "\n",
    "Bag-of-words(BoW) is one of the  effective ways to represent the text for machine learning is using the Bag-of-Words. When using this representation the algorithm simply seeks to know the number of times a given words is present with a body of text. We discard the structure of the text such as chapters,paragraphs,sentences and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec7dc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us consider the list of tokens created from the example given in tokenization section. \n",
    "# \n",
    "sample_sent_tokens=['I am a student from the University of Alabama.', \n",
    "                     'I \\nwas born in Ontario, Canada and I am a huge fan of the United States.', \n",
    "                     'I am going to get a degree in Philosophy to improve my chances of \\nbecoming a Philosophy professor.', \n",
    "                     'I have been working towards this goal\\nfor 4 years.', 'I am currently enrolled in a PhD program.', \n",
    "                     'It is very difficult, \\nbut I am confident that it will be a good decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae9a560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c = CountVectorizer(stop_words='english', token_pattern=r'\\w+')\n",
    "converted_data = c.fit_transform(sample_sent_tokens).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f836ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 2 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(converted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae160e8",
   "metadata": {},
   "source": [
    "Our converted data set is of arrary 6$\\times$50, which means we have six sentences where each of the sentences has 50 features. Let us look at the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c51f0efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', 'alabama', 'born', 'canada', 'chances', 'confident', 'currently', 'decision', 'degree', 'difficult', 'enrolled', 'fan', 'goal', 'going', 'good', 'huge', 'improve', 'ontario', 'phd', 'philosophy', 'professor', 'program', 'states', 'student', 'united', 'university', 'working', 'years']\n"
     ]
    }
   ],
   "source": [
    "print(c.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff293f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T19:21:23.142697Z",
     "start_time": "2022-11-02T19:21:23.131697Z"
    }
   },
   "source": [
    "# Exercise for Students: NLP for SMS SPAM  DETECTION using MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdeb52",
   "metadata": {},
   "source": [
    "In this exercise, we are going to use the dataset from UCI ML repository,https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. The dataset is already available in the jupyter notebook text/spam path. The datasets consists of different SMS messages labeled as 'ham'(genuine) and 'spam'. Your task would be to preprocess the text data , create a MLP model and train the data.\n",
    "\n",
    "STEPS TO DO in the following order\n",
    "- read the file spam.csv located at path text/spam.csv\n",
    "- Use label encoder to encode the target variables value 'ham' and 'spam'\n",
    "- tokenize the the text amd conver them to lower case letters.\n",
    "- Remove the stop words\n",
    "- Lemmatize the text\n",
    "- Use count vectorizer as word embedding\n",
    "- Split the dataset into training and test\n",
    "- Build a MLP model from given specification\n",
    "     <table>\n",
    "      <thead>\n",
    "        <tr>\n",
    "          <th>Layers</th>\n",
    "          <th>Neurons</th>\n",
    "          <th>Activation Function</th>\n",
    "        </tr>\n",
    "      </thead>\n",
    "      <tbody>\n",
    "        <tr>\n",
    "          <td>Dense Layer-I</td>\n",
    "          <td>512</td>\n",
    "          <td>tanh</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "         <td>Dropoutlayer(rate=0.5)</td>\n",
    "          <td>-</td>\n",
    "          <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "         <td>Dense Layer-II</td>\n",
    "          <td>2</td>\n",
    "          <td>-</td>\n",
    "        </tr> \n",
    "      </tbody>\n",
    "    </table>\n",
    "- Fit the data with above given model and compare predicted and test labels.\n",
    "\n",
    "<b>HINT: You can use the idea from the chapter 1 Introduction to Deep Learning for this exercise</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8381fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution: Not to be disclosed during teaching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5650c62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:01.167150Z",
     "start_time": "2022-11-02T20:23:01.155150Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8b6d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:01.480974Z",
     "start_time": "2022-11-02T20:23:01.455980Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('text/spam.csv',encoding=\"ISO-8859-1\").dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d41cfde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:01.778640Z",
     "start_time": "2022-11-02T20:23:01.769638Z"
    }
   },
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "df['v1']=le.fit_transform(df.v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a43d476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:01.965818Z",
     "start_time": "2022-11-02T20:23:01.955822Z"
    }
   },
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99eacd52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:03.502362Z",
     "start_time": "2022-11-02T20:23:02.141153Z"
    }
   },
   "outputs": [],
   "source": [
    "df['wrd_tokenize']=df['v2'].apply(lambda x:[word.lower() # change to lower case\n",
    "                                            for sent in nltk.sent_tokenize(x) \n",
    "                                            for word in nltk.word_tokenize(sent)])\n",
    "df['wrd_stop']=df['wrd_tokenize'].apply(lambda x:[wrd  for wrd in x if wrd not in  stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48194b73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:03.518249Z",
     "start_time": "2022-11-02T20:23:03.503274Z"
    }
   },
   "outputs": [],
   "source": [
    "wrd_lmtz=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "864b0f51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:03.754436Z",
     "start_time": "2022-11-02T20:23:03.520211Z"
    }
   },
   "outputs": [],
   "source": [
    "df['wrd_lmtz']=df['wrd_stop'].apply(lambda x:[wrd_lmtz.lemmatize(wrd_stp)  for wrd_stp in x ])\n",
    "df['clean_join'] =df['wrd_lmtz'].apply(lambda x:'-'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29dd004d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:03.769239Z",
     "start_time": "2022-11-02T20:23:03.756215Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df[['clean_join','v1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c638ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T19:28:02.521879Z",
     "start_time": "2022-11-02T19:28:02.509880Z"
    }
   },
   "source": [
    "## WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb28b00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:04.253967Z",
     "start_time": "2022-11-02T20:23:04.189759Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt_vtr=CountVectorizer()\n",
    "X=cnt_vtr.fit_transform(df['clean_join'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52467c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:04.677753Z",
     "start_time": "2022-11-02T20:23:04.667718Z"
    }
   },
   "outputs": [],
   "source": [
    "y=df.v1.to_numpy()\n",
    "num_of_classes=len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4ab9045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:23:04.898195Z",
     "start_time": "2022-11-02T20:23:04.878199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffb2e5ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:24:31.019157Z",
     "start_time": "2022-11-02T20:24:30.776775Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8daa7ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:24:31.521129Z",
     "start_time": "2022-11-02T20:24:31.502141Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val=X_test[:1800,:]\n",
    "y_val=y_test[:1800]\n",
    "X_test=X_test[1800:,:]\n",
    "y_test=y_test[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54b1360c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:24:32.003188Z",
     "start_time": "2022-11-02T20:24:31.992189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 8114), (1800,), (39, 8114), (39,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape,y_val.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ad0b648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:24:44.153123Z",
     "start_time": "2022-11-02T20:24:44.147123Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets,layers,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bfa2dc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:38:57.988192Z",
     "start_time": "2022-11-02T20:38:57.966194Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_feats=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6e07b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:24:46.736322Z",
     "start_time": "2022-11-02T20:24:46.684161Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=10\n",
    "\n",
    "model = model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(number_of_feats,)))\n",
    "model.add(tf.keras.layers.Dense(512, activation='tanh') )\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(num_of_classes)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f27279a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:24:47.160491Z",
     "start_time": "2022-11-02T20:24:47.152492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               4154880   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 4,155,906\n",
      "Trainable params: 4,155,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13e7b7a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:25:05.978332Z",
     "start_time": "2022-11-02T20:24:47.739017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 2s 15ms/step - loss: 0.1542 - accuracy: 0.9590 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0209 - accuracy: 0.9938 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 2s 14ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 2s 15ms/step - loss: 9.5273e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 2s 19ms/step - loss: 5.7170e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 3s 27ms/step - loss: 3.8082e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 2s 16ms/step - loss: 2.9072e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 2s 16ms/step - loss: 2.2199e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 2s 16ms/step - loss: 1.8491e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c82ab03940>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f476026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-02T20:35:51.961906Z",
     "start_time": "2022-11-02T20:35:50.864649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   1----------------->pred_label   1\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   1----------------->pred_label   1\n",
      "true_label   1----------------->pred_label   1\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   1----------------->pred_label   1\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   1----------------->pred_label   1\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   1----------------->pred_label   1\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   0----------------->pred_label   0\n",
      "true_label   1----------------->pred_label   1\n"
     ]
    }
   ],
   "source": [
    "for x_tst,y_tst   in zip(X_test,y_test):\n",
    "    y_pred=np.argmax(model.predict(x_tst.reshape(1,-1)))\n",
    "    print(f'true_label   {y_tst}----------------->pred_label   {y_pred}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
